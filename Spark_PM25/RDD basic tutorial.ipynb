{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of RDD basic tutorial.ipynb","provenance":[{"file_id":"https://github.com/UDICatNCHU/SparkTutorial/blob/master/(Spark%20Tutorial)%20%E4%B8%8A%E8%AA%B2%E8%AC%9B%E7%BE%A9%20RDD%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5.ipynb","timestamp":1571108764923}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"cF0hz0LrunvS","colab_type":"code","outputId":"4c51a5bb-1e06-4661-d734-8936cd6ad2ec","executionInfo":{"status":"ok","timestamp":1571102565400,"user_tz":-480,"elapsed":16752,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# 環境初始化 (大約三至五分鐘)\n","! wget -O init_env.sh https://www.dropbox.com/s/6bnwn8u2hz19s59/init_env.sh && \\\n","bash init_env.sh"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2019-10-15 01:22:49--  https://www.dropbox.com/s/6bnwn8u2hz19s59/init_env.sh\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.82.1, 2620:100:6032:1::a27d:5201\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.82.1|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/6bnwn8u2hz19s59/init_env.sh [following]\n","--2019-10-15 01:22:54--  https://www.dropbox.com/s/raw/6bnwn8u2hz19s59/init_env.sh\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc404dc934d229375e70cb1f2d61.dl.dropboxusercontent.com/cd/0/inline/AqcsA7aWnl32bApWSigSY1E_G_MYFOTiU6VB2DtZiphTDhxYPbS8gk7S-edNlawOyLIhd59lVAmaDsifzCLXRPvpoDHayqvktxJYzlL6aIDzsw/file# [following]\n","--2019-10-15 01:22:55--  https://uc404dc934d229375e70cb1f2d61.dl.dropboxusercontent.com/cd/0/inline/AqcsA7aWnl32bApWSigSY1E_G_MYFOTiU6VB2DtZiphTDhxYPbS8gk7S-edNlawOyLIhd59lVAmaDsifzCLXRPvpoDHayqvktxJYzlL6aIDzsw/file\n","Resolving uc404dc934d229375e70cb1f2d61.dl.dropboxusercontent.com (uc404dc934d229375e70cb1f2d61.dl.dropboxusercontent.com)... 162.125.82.6, 2620:100:6032:6::a27d:5206\n","Connecting to uc404dc934d229375e70cb1f2d61.dl.dropboxusercontent.com (uc404dc934d229375e70cb1f2d61.dl.dropboxusercontent.com)|162.125.82.6|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 336 [text/plain]\n","Saving to: ‘init_env.sh’\n","\n","init_env.sh         100%[===================>]     336  --.-KB/s    in 0s      \n","\n","2019-10-15 01:22:55 (55.6 MB/s) - ‘init_env.sh’ saved [336/336]\n","\n","--2019-10-15 01:22:55--  https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0-bin-hadoop2.7.tgz\n","Resolving d3kbcqa49mib13.cloudfront.net (d3kbcqa49mib13.cloudfront.net)... 52.84.225.188, 52.84.225.215, 52.84.225.35, ...\n","Connecting to d3kbcqa49mib13.cloudfront.net (d3kbcqa49mib13.cloudfront.net)|52.84.225.188|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 203728858 (194M) [application/x-tar]\n","Saving to: ‘spark-2.2.0-bin-hadoop2.7.tgz’\n","\n","spark-2.2.0-bin-had 100%[===================>] 194.29M  55.2MB/s    in 3.8s    \n","\n","2019-10-15 01:22:59 (50.9 MB/s) - ‘spark-2.2.0-bin-hadoop2.7.tgz’ saved [203728858/203728858]\n","\n","spark-2.2.0-bin-hadoop2.7/\n","spark-2.2.0-bin-hadoop2.7/NOTICE\n","spark-2.2.0-bin-hadoop2.7/jars/\n","spark-2.2.0-bin-hadoop2.7/jars/parquet-common-1.8.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/bonecp-0.8.0.RELEASE.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-net-2.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/javax.servlet-api-3.1.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-annotations-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-hdfs-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/oro-2.0.8.jar\n","spark-2.2.0-bin-hadoop2.7/jars/xercesImpl-2.9.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/antlr-runtime-3.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/machinist_2.11-0.6.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spire_2.11-0.13.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/parquet-hadoop-1.8.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-sketch_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/stream-2.7.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/kryo-shaded-3.0.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/metrics-jvm-3.1.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-common-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/RoaringBitmap-0.5.11.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-auth-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/activation-1.1.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jta-1.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/datanucleus-core-3.2.10.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jersey-container-servlet-2.22.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jersey-guava-2.22.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jets3t-0.9.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jetty-util-6.1.26.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-compress-1.4.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jersey-container-servlet-core-2.22.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-beanutils-1.7.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hk2-utils-2.4.0-b34.jar\n","spark-2.2.0-bin-hadoop2.7/jars/parquet-format-2.3.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/avro-1.7.7.jar\n","spark-2.2.0-bin-hadoop2.7/jars/datanucleus-api-jdo-3.2.6.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jline-2.12.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/metrics-core-3.1.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/java-xmlbuilder-1.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/stax-api-1.0-2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hk2-locator-2.4.0-b34.jar\n","spark-2.2.0-bin-hadoop2.7/jars/parquet-hadoop-bundle-1.6.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jsp-api-2.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/xmlenc-0.52.jar\n","spark-2.2.0-bin-hadoop2.7/jars/xbean-asm5-shaded-4.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jackson-core-asl-1.9.13.jar\n","spark-2.2.0-bin-hadoop2.7/jars/shapeless_2.11-2.3.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-collections-3.2.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/javax.inject-1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-sql_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/json4s-jackson_2.11-3.2.11.jar\n","spark-2.2.0-bin-hadoop2.7/jars/json4s-ast_2.11-3.2.11.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-codec-1.10.jar\n","spark-2.2.0-bin-hadoop2.7/jars/leveldbjni-all-1.8.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-httpclient-3.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/aopalliance-repackaged-2.4.0-b34.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-server-common-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/minlog-1.3.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/javolution-5.5.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/datanucleus-rdbms-3.2.9.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jersey-common-2.22.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-graphx_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/api-asn1-api-1.0.0-M20.jar\n","spark-2.2.0-bin-hadoop2.7/jars/apacheds-i18n-2.0.0-M15.jar\n","spark-2.2.0-bin-hadoop2.7/jars/validation-api-1.1.0.Final.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-dbcp-1.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-pool-1.5.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-network-common_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/pmml-schema-1.2.15.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-shuffle-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/joda-time-2.9.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-hive-thriftserver_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spire-macros_2.11-0.13.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/curator-recipes-2.6.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jersey-server-2.22.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/htrace-core-3.1.0-incubating.jar\n","spark-2.2.0-bin-hadoop2.7/jars/httpclient-4.5.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-mllib-local_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/snappy-0.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/breeze-macros_2.11-0.13.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jackson-jaxrs-1.9.13.jar\n","spark-2.2.0-bin-hadoop2.7/jars/eigenbase-properties-1.1.5.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-tags_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jackson-databind-2.6.5.jar\n","spark-2.2.0-bin-hadoop2.7/jars/curator-client-2.6.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-catalyst_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/paranamer-2.6.jar\n","spark-2.2.0-bin-hadoop2.7/jars/opencsv-2.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/json4s-core_2.11-3.2.11.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hive-metastore-1.2.1.spark2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-digester-1.8.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jsr305-1.3.9.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-repl_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jetty-6.1.26.jar\n","spark-2.2.0-bin-hadoop2.7/jars/pyrolite-4.13.jar\n","spark-2.2.0-bin-hadoop2.7/jars/log4j-1.2.17.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-server-web-proxy-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/calcite-avatica-1.2.0-incubating.jar\n","spark-2.2.0-bin-hadoop2.7/jars/scala-xml_2.11-1.0.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-network-shuffle_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/calcite-linq4j-1.2.0-incubating.jar\n","spark-2.2.0-bin-hadoop2.7/jars/compress-lzf-1.0.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jtransforms-2.4.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/apache-log4j-extras-1.2.17.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jcl-over-slf4j-1.7.16.jar\n","spark-2.2.0-bin-hadoop2.7/jars/guice-3.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/gson-2.2.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/httpcore-4.4.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/protobuf-java-2.5.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-core-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-yarn_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-hive_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-lang-2.6.jar\n","spark-2.2.0-bin-hadoop2.7/jars/stax-api-1.0.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/javax.annotation-api-1.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/netty-all-4.0.43.Final.jar\n","spark-2.2.0-bin-hadoop2.7/jars/curator-framework-2.6.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-io-2.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/avro-ipc-1.7.7.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hive-beeline-1.2.1.spark2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/mesos-1.0.0-shaded-protobuf.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jackson-annotations-2.6.5.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jersey-media-jaxb-2.22.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-common-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jackson-mapper-asl-1.9.13.jar\n","spark-2.2.0-bin-hadoop2.7/jars/parquet-jackson-1.8.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-client-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jackson-module-paranamer-2.6.5.jar\n","spark-2.2.0-bin-hadoop2.7/jars/aopalliance-1.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/super-csv-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/janino-3.0.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/antlr4-runtime-4.5.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jpam-1.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-lang3-3.5.jar\n","spark-2.2.0-bin-hadoop2.7/jars/javassist-3.18.1-GA.jar\n","spark-2.2.0-bin-hadoop2.7/jars/bcprov-jdk15on-1.51.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-launcher_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/javax.ws.rs-api-2.0.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-jobclient-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/metrics-graphite-3.1.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jackson-xc-1.9.13.jar\n","spark-2.2.0-bin-hadoop2.7/jars/lz4-1.3.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/core-1.1.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-mesos_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/antlr-2.7.7.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-client-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/mx4j-3.0.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-logging-1.1.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-beanutils-core-1.8.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/libfb303-0.9.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/libthrift-0.9.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jaxb-api-2.2.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hive-exec-1.2.1.spark2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/calcite-core-1.2.0-incubating.jar\n","spark-2.2.0-bin-hadoop2.7/jars/parquet-encoding-1.8.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-mllib_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/api-util-1.0.0-M20.jar\n","spark-2.2.0-bin-hadoop2.7/jars/apacheds-kerberos-codec-2.0.0-M15.jar\n","spark-2.2.0-bin-hadoop2.7/jars/mail-1.4.7.jar\n","spark-2.2.0-bin-hadoop2.7/jars/stringtemplate-3.2.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/guice-servlet-3.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-crypto-1.0.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/JavaEWAH-0.3.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/metrics-json-3.1.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jdo-api-3.0.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/scalap-2.11.8.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hive-cli-1.2.1.spark2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/zookeeper-3.4.6.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jackson-module-scala_2.11-2.6.5.jar\n","spark-2.2.0-bin-hadoop2.7/jars/ivy-2.4.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/py4j-0.10.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/arpack_combined_all-0.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-unsafe_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/macro-compat_2.11-1.1.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jul-to-slf4j-1.7.16.jar\n","spark-2.2.0-bin-hadoop2.7/jars/pmml-model-1.2.15.jar\n","spark-2.2.0-bin-hadoop2.7/jars/parquet-column-1.8.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/javax.inject-2.4.0-b34.jar\n","spark-2.2.0-bin-hadoop2.7/jars/breeze_2.11-0.13.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-cli-1.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/chill-java-0.8.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/avro-mapred-1.7.7-hadoop2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/snappy-java-1.1.2.6.jar\n","spark-2.2.0-bin-hadoop2.7/jars/base64-2.3.8.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-compiler-3.0.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/slf4j-api-1.7.16.jar\n","spark-2.2.0-bin-hadoop2.7/jars/derby-10.12.1.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-app-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/objenesis-2.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jodd-core-3.5.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jersey-client-2.22.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/ST4-4.0.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/univocity-parsers-2.2.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/scala-parser-combinators_2.11-1.0.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jackson-core-2.6.5.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-math3-3.4.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/xz-1.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/scala-reflect-2.11.8.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-configuration-1.6.jar\n","spark-2.2.0-bin-hadoop2.7/jars/scala-compiler-2.11.8.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-api-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hk2-api-2.4.0-b34.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-core_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/guava-14.0.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/netty-3.9.9.Final.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-streaming_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/chill_2.11-0.8.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/scala-library-2.11.8.jar\n","spark-2.2.0-bin-hadoop2.7/jars/osgi-resource-locator-1.0.1.jar\n","spark-2.2.0-bin-hadoop2.7/python/\n","spark-2.2.0-bin-hadoop2.7/python/run-tests.py\n","spark-2.2.0-bin-hadoop2.7/python/test_support/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/hello/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/hello/sub_hello/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/hello/sub_hello/sub_hello.txt\n","spark-2.2.0-bin-hadoop2.7/python/test_support/hello/hello.txt\n","spark-2.2.0-bin-hadoop2.7/python/test_support/userlibrary.py\n","spark-2.2.0-bin-hadoop2.7/python/test_support/userlib-0.1.zip\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/people.json\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/people1.json\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/_SUCCESS\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/streaming/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/streaming/text-test.txt\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/text-test.txt\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/people_array.json\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/.part-r-00008.gz.parquet.crc\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/part-r-00008.gz.parquet\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_SUCCESS\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/part-r-00005.gz.parquet\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/.part-r-00005.gz.parquet.crc\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00002.gz.parquet\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00002.gz.parquet.crc\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00004.gz.parquet\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00004.gz.parquet.crc\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/.part-r-00007.gz.parquet.crc\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/part-r-00007.gz.parquet\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_metadata\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_common_metadata\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/ages_newlines.csv\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/ages.csv\n","spark-2.2.0-bin-hadoop2.7/python/test_support/SimpleHTTPServer.py\n","spark-2.2.0-bin-hadoop2.7/python/pylintrc\n","spark-2.2.0-bin-hadoop2.7/python/docs/\n","spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.ml.rst\n","spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.streaming.rst\n","spark-2.2.0-bin-hadoop2.7/python/docs/conf.py\n","spark-2.2.0-bin-hadoop2.7/python/docs/_templates/\n","spark-2.2.0-bin-hadoop2.7/python/docs/_templates/layout.html\n","spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.rst\n","spark-2.2.0-bin-hadoop2.7/python/docs/make.bat\n","spark-2.2.0-bin-hadoop2.7/python/docs/epytext.py\n","spark-2.2.0-bin-hadoop2.7/python/docs/make2.bat\n","spark-2.2.0-bin-hadoop2.7/python/docs/index.rst\n","spark-2.2.0-bin-hadoop2.7/python/docs/_static/\n","spark-2.2.0-bin-hadoop2.7/python/docs/_static/pyspark.js\n","spark-2.2.0-bin-hadoop2.7/python/docs/_static/pyspark.css\n","spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.sql.rst\n","spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.mllib.rst\n","spark-2.2.0-bin-hadoop2.7/python/docs/Makefile\n","spark-2.2.0-bin-hadoop2.7/python/.gitignore\n","spark-2.2.0-bin-hadoop2.7/python/MANIFEST.in\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/status.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/version.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/conf.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/base.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/evaluation.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/util.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/classification.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/regression.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/tests.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/tuning.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/common.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/stat.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/linalg/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/linalg/__init__.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/pipeline.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/feature.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/clustering.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/recommendation.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/__init__.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/param/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/param/_shared_params_code_gen.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/param/shared.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/param/__init__.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/fpm.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/statcounter.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/profiler.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/serializers.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/traceback_utils.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/shell.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/conf.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/session.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/window.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/tests.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/utils.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/group.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/types.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/catalog.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/context.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/dataframe.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/column.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/streaming.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/__init__.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/readwriter.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/functions.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/taskcontext.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/util.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/python/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/python/pyspark/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/python/pyspark/shell.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/daemon.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/tests.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/resultiterable.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/heapq3.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/broadcast.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/shuffle.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/cloudpickle.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/accumulators.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/java_gateway.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/util.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/listener.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/tests.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/flume.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/kafka.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/kinesis.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/dstream.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/context.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/__init__.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/context.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/storagelevel.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/__init__.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/join.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/tree.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/evaluation.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/util.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/classification.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/regression.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/tests.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/common.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/linalg/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/linalg/__init__.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/linalg/distributed.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/feature.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/clustering.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/recommendation.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/__init__.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/_statistics.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/KernelDensity.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/test.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/distribution.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/random.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/__init__.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/fpm.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/rdd.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/rddsampler.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/worker.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/files.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/find_spark_home.py\n","spark-2.2.0-bin-hadoop2.7/python/setup.cfg\n","spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/SOURCES.txt\n","spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/requires.txt\n","spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/PKG-INFO\n","spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/dependency_links.txt\n","spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/top_level.txt\n","spark-2.2.0-bin-hadoop2.7/python/run-tests\n","spark-2.2.0-bin-hadoop2.7/python/dist/\n","spark-2.2.0-bin-hadoop2.7/python/setup.py\n","spark-2.2.0-bin-hadoop2.7/python/lib/\n","spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip\n","spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip\n","spark-2.2.0-bin-hadoop2.7/python/lib/PY4J_LICENSE.txt\n","spark-2.2.0-bin-hadoop2.7/python/README.md\n","spark-2.2.0-bin-hadoop2.7/RELEASE\n","spark-2.2.0-bin-hadoop2.7/sbin/\n","spark-2.2.0-bin-hadoop2.7/sbin/start-mesos-shuffle-service.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/start-mesos-dispatcher.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/spark-daemon.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/stop-slaves.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/stop-thriftserver.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/stop-shuffle-service.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/stop-history-server.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/spark-config.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/start-history-server.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/start-thriftserver.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/start-shuffle-service.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/spark-daemons.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/start-all.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/stop-master.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/stop-mesos-dispatcher.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/stop-slave.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/start-slave.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/stop-mesos-shuffle-service.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/start-slaves.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/stop-all.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/slaves.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/start-master.sh\n","spark-2.2.0-bin-hadoop2.7/examples/\n","spark-2.2.0-bin-hadoop2.7/examples/src/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/RSparkSQLExample.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/fpm.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/gbt.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/isoreg.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/randomForest.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/als.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/kstest.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/gaussianMixture.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/ml.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/survreg.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/glm.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/kmeans.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/naiveBayes.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/svmLinear.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/lda.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/bisectingKmeans.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/mlp.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/logit.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/data-manipulation.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/dataframe.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/streaming/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/streaming/structured_network_wordcount.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/status_api_demo.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/linearsvc.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/random_forest_regressor_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/stopwords_remover_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/min_hash_lsh_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/standard_scaler_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/fpgrowth_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/vector_assembler_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/max_abs_scaler_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/binarizer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/imputer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/bisecting_k_means_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/cross_validator.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_summary_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/dataframe_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/naive_bayes_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/generalized_linear_regression_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_regression_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/count_vectorizer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/one_vs_rest_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/linear_regression_with_elastic_net.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/index_to_string_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/sql_transformer.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/correlation_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/dct_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/isotonic_regression_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/vector_slicer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/multilayer_perceptron_classification.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/kmeans_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/pca_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/lda_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/polynomial_expansion_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/onehot_encoder_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/aft_survival_regression.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/rformula_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_classification_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/estimator_transformer_param_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/tokenizer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/min_max_scaler_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/pipeline_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/random_forest_classifier_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/string_indexer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/als_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/n_gram_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/vector_indexer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/normalizer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/quantile_discretizer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/train_validation_split.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/chisq_selector_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/gaussian_mixture_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/bucketizer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/elementwise_product_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/tf_idf_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/word2vec_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/chi_square_test_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/pagerank.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/hive.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/basic.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/datasource.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/pi.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/logistic_regression.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/network_wordjoinsentiments.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/sql_network_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/queue_stream.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/network_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/kafka_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/stateful_network_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/direct_kafka_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/hdfs_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/recoverable_network_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/flume_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/transitive_closure.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/kmeans.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/avro_inputformat.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/linear_regression_with_sgd_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/svd_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/recommendation_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/regression_metrics_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/standard_scaler_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/kernel_density_estimation_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_regression_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/sampled_rdds.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/fpgrowth_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/streaming_k_means_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/bisecting_k_means_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_classification_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/multi_class_metrics_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/correlations_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/pca_rowmatrix_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/naive_bayes_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_regression_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/ranking_metrics_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_model.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_regression_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/power_iteration_clustering_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/binary_classification_metrics_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/isotonic_regression_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/word2vec.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/summary_statistics_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/multi_label_metrics_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/kmeans.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_classification_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/random_rdd_generation.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/svm_with_sgd_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/streaming_linear_regression_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/stratified_sampling_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/normalizer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_classification_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/elementwise_product_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/tf_idf_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/word2vec_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/correlations.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/k_means_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/parquet_inputformat.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/als.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sort.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaTC.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaFlumeEventCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaKafkaWordCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRegressionMetricsExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLinearRegressionWithSGDExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/FlumePollingEventCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/FlumeEventCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/KafkaWordCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RegressionMetricsExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LinearRegressionWithSGDExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LinearRegression.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/people.json\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/employees.json\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/people.txt\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/full_user.avsc\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/kv1.txt\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/users.parquet\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/user.avsc\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/users.avro\n","spark-2.2.0-bin-hadoop2.7/examples/jars/\n","spark-2.2.0-bin-hadoop2.7/examples/jars/scopt_2.11-3.3.0.jar\n","spark-2.2.0-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/data/\n","spark-2.2.0-bin-hadoop2.7/data/graphx/\n","spark-2.2.0-bin-hadoop2.7/data/graphx/followers.txt\n","spark-2.2.0-bin-hadoop2.7/data/graphx/users.txt\n","spark-2.2.0-bin-hadoop2.7/data/streaming/\n","spark-2.2.0-bin-hadoop2.7/data/streaming/AFINN-111.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/\n","spark-2.2.0-bin-hadoop2.7/data/mllib/pagerank_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/kmeans_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/streaming_kmeans_data_test.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_lda_libsvm_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_kmeans_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/pic_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_isotonic_regression_libsvm_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/als/\n","spark-2.2.0-bin-hadoop2.7/data/mllib/als/test.data\n","spark-2.2.0-bin-hadoop2.7/data/mllib/als/sample_movielens_ratings.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_fpgrowth.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/ridge-data/\n","spark-2.2.0-bin-hadoop2.7/data/mllib/ridge-data/lpsa.data\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_multiclass_classification_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_linear_regression_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_binary_classification_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_lda_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_movielens_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_svm_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/gmm_data.txt\n","spark-2.2.0-bin-hadoop2.7/R/\n","spark-2.2.0-bin-hadoop2.7/R/lib/\n","spark-2.2.0-bin-hadoop2.7/R/lib/sparkr.zip\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/groupBy.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/covar_pop.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sampleBy.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sql.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/year.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tan.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GBTRegressionModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/last_day.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sign.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hint.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/randn.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/orderBy.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/otherwise.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/AFTSurvivalRegressionModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hashCode.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/bin.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dim.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.svmLinear.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/minute.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/createExternalTable-deprecated.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/distinct.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.conf.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/print.jobj.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/md5.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cbrt.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.ml.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dapplyCollect.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/acos.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tables.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/NaiveBayesModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sum.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/structType.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hex.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/isLocal.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.jdbc.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/from_utc_timestamp.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tableNames.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/createDataFrame.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/isStreaming.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/toJSON.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.getSparkFiles.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/except.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/LDAModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/months_between.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark_partition_id.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.parquet.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sumDistinct.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/awaitTermination.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/BisectingKMeansModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sha2.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/abs.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/date_format.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/withColumn.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dayofyear.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sort_array.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/storageLevel.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/setCurrentDatabase.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ceil.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/floor.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/stddev_pop.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sd.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/print.structType.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.survreg.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/predict.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/count.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/unhex.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/mean.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/instr.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/from_unixtime.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/saveAsTable.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ltrim.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkRHive.init-deprecated.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.parquet.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/match.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/is.nan.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.ml.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lag.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.json.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/unpersist.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/corr.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.callJStatic.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rint.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/LinearSVCModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.gbt.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/RandomForestClassificationModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/persist.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/var_pop.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/selectExpr.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/crc32.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/expr.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.callJMethod.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/with.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/generateAliasesForIntersectedCols.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GeneralizedLinearRegressionModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/IsotonicRegressionModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/setLogLevel.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/shiftRightUnsigned.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/base64.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/array_contains.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/expm1.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.orc.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.version.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/insertInto.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/SparkDataFrame.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/merge.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dayofmonth.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/listDatabases.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/summarize.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/format_number.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/from_json.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dropDuplicates.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cache.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.text.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/approxCountDistinct.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkRSQL.init-deprecated.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/LogisticRegressionModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/stddev_samp.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/pivot.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/showDF.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/between.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/struct.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/subset.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/posexplode.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lit.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/shiftLeft.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/glm.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hypot.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/recoverPartitions.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.session.stop.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/translate.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/drop.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GBTClassificationModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/shiftRight.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/regexp_replace.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/randomSplit.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/length.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rowsBetween.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.jdbc.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/schema.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/toRadians.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/filter.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/bround.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/createOrReplaceTempView.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cancelJobGroup.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/second.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/upper.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/head.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/limit.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/concat_ws.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/when.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/FPGrowthModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/install.spark.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.newJObject.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dropTempView.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/unbase64.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/soundex.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/structField.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.addFile.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.bisectingKmeans.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cacheTable.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cosh.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.mlp.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ntile.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/atan2.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.kstest.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dtypes.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/reverse.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sinh.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.lda.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/createTable.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/negate.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/asin.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hash.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rank.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/toDegrees.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/columns.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/columnfunctions.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/substring_index.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/to_timestamp.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.naiveBayes.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/atan.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.isoreg.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/factorial.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/countDistinct.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/quarter.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/setCheckpointDir.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/least.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.text.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/windowOrderBy.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dapply.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/coalesce.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/refreshByPath.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cume_dist.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dense_rank.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/freqItems.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/getNumPartitions.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/KMeansModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.session.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/arrange.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.stream.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/encode.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.glm.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/isActive.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/crossJoin.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rpad.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/uncacheTable.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/size.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/conv.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/log10.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/collect.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.stream.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/format_string.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/windowPartitionBy.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/union.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/stopQuery.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dropTempTable-deprecated.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/endsWith.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/startsWith.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/nanvl.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/mutate.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/explain.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/R.css\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cov.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/var_samp.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/log2.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/registerTempTable-deprecated.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lastProgress.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/attach.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/min.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ncol.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/month.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/window.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/partitionBy.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/percent_rank.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/listFunctions.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/to_utc_timestamp.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/crosstab.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/take.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/exp.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/to_json.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/column.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ifelse.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GaussianMixtureModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.logit.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/show.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/setJobGroup.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/KSTest-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/clearJobGroup.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/last.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/printSchema.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rename.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rbind.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/over.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/MultilayerPerceptronClassificationModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/coltypes.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/datediff.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lead.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/summary.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/WindowSpec.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/unix_timestamp.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tanh.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/listColumns.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/to_date.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/00Index.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.uiWebUrl.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/max.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/var.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/print.structField.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.als.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/alias.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sha1.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/status.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.orc.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/currentDatabase.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.df.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/round.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.init-deprecated.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/refreshTable.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/nrow.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/pmod.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/intersect.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rtrim.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/substr.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/log.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/levenshtein.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/monotonically_increasing_id.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/checkpoint.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/decode.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.lapply.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.json.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/RandomForestRegressionModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/trim.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/select.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/regexp_extract.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/as.data.frame.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rangeBetween.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/queryName.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sample.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lower.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/repartition.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/concat.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cast.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/date_add.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.fpGrowth.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hour.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/initcap.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/explode.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/add_months.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/bitwiseNOT.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/approxQuantile.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/kurtosis.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/greatest.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/first.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.df.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rand.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/next_day.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/clearCache.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/nafunctions.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/row_number.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lpad.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/skewness.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/gapplyCollect.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/locate.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/avg.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sqrt.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cos.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/log1p.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/str.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/StreamingQuery.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ALSModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ascii.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/listTables.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sin.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/histogram.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/weekofyear.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GroupedData.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/fitted.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/date_sub.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tableToDF.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.getSparkFilesRootDirectory.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/join.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/gapply.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.randomForest.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.kmeans.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.gaussianMixture.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/INDEX\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/R/\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdx\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/R/SparkR\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdb\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/aliases.rds\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/paths.rds\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdx\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/AnIndex\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdb\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/DESCRIPTION\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/hsearch.rds\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/nsInfo.rds\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/package.rds\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/links.rds\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/Rd.rds\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/profile/\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/profile/general.R\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/profile/shell.R\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/worker/\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/worker/daemon.R\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/worker/worker.R\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/NAMESPACE\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/tests/\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/tests/testthat/\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/tests/testthat/test_basic.R\n","spark-2.2.0-bin-hadoop2.7/licenses/\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-slf4j.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-scalacheck.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-py4j.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-paranamer.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-netlib.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-Mockito.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-minlog.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-graphlib-dot.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-d3.min.js.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-f2j.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-junit-interface.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-boto.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-spire.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-pyrolite.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-xmlenc.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-scala.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-SnapTree.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-AnchorJS.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-jline.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-heapq.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-cloudpickle.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-reflectasm.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-jpmml-model.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-modernizr.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-javolution.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-jbcrypt.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-sbt-launch-lib.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-postgresql.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-kryo.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-DPark.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-scopt.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-sorttable.js.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-dagre-d3.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-jquery.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-protobuf.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-antlr.txt\n","spark-2.2.0-bin-hadoop2.7/conf/\n","spark-2.2.0-bin-hadoop2.7/conf/fairscheduler.xml.template\n","spark-2.2.0-bin-hadoop2.7/conf/metrics.properties.template\n","spark-2.2.0-bin-hadoop2.7/conf/spark-env.sh.template\n","spark-2.2.0-bin-hadoop2.7/conf/log4j.properties.template\n","spark-2.2.0-bin-hadoop2.7/conf/docker.properties.template\n","spark-2.2.0-bin-hadoop2.7/conf/slaves.template\n","spark-2.2.0-bin-hadoop2.7/conf/spark-defaults.conf.template\n","spark-2.2.0-bin-hadoop2.7/LICENSE\n","spark-2.2.0-bin-hadoop2.7/bin/\n","spark-2.2.0-bin-hadoop2.7/bin/spark-shell\n","spark-2.2.0-bin-hadoop2.7/bin/spark-submit.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/spark-shell2.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/pyspark\n","spark-2.2.0-bin-hadoop2.7/bin/sparkR.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/spark-class2.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/run-example.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/spark-submit2.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/spark-class\n","spark-2.2.0-bin-hadoop2.7/bin/spark-submit\n","spark-2.2.0-bin-hadoop2.7/bin/spark-sql\n","spark-2.2.0-bin-hadoop2.7/bin/find-spark-home\n","spark-2.2.0-bin-hadoop2.7/bin/run-example\n","spark-2.2.0-bin-hadoop2.7/bin/beeline\n","spark-2.2.0-bin-hadoop2.7/bin/pyspark2.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/spark-shell.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/spark-class.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/pyspark.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/sparkR\n","spark-2.2.0-bin-hadoop2.7/bin/beeline.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/sparkR2.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/load-spark-env.sh\n","spark-2.2.0-bin-hadoop2.7/bin/load-spark-env.cmd\n","spark-2.2.0-bin-hadoop2.7/yarn/\n","spark-2.2.0-bin-hadoop2.7/yarn/spark-2.2.0-yarn-shuffle.jar\n","spark-2.2.0-bin-hadoop2.7/README.md\n","環境初始化完畢\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2oBsLO2DHb2g","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"WDrjL-AdO0ba","colab_type":"code","outputId":"72cd9153-dff2-43a0-c08d-491dee5e09fd","executionInfo":{"status":"ok","timestamp":1571102567681,"user_tz":-480,"elapsed":19021,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!which python"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/bin/python\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5e-bX8_hN1yT","colab_type":"code","outputId":"f79cf6f3-82a1-4cc4-a995-71e61706f1b8","executionInfo":{"status":"ok","timestamp":1571102570257,"user_tz":-480,"elapsed":21586,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!/usr/local/bin/python -V"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Python 3.6.8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f1m9p6bAuwgY","colab_type":"code","colab":{}},"source":["import os, sys\n","os.environ['SPARK_HOME'] = \"/usr/local/spark\"\n","# os.environ['PYSPARK_PYTHON'] = \"/usr/bin/python\"\n","os.environ['PYSPARK_PYTHON'] = \"/usr/local/bin/python\"\n","sys.path.append(\"/usr/local/spark/python/\")\n","sys.path.append(\"/usr/local/spark/python/lib/pyspark.zip\")\n","sys.path.append(\"/usr/local/spark/python/lib/py4j-0.10.4-src.zip\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Siw9OMRLu0uA","colab_type":"code","colab":{}},"source":["from pyspark import SparkContext\n","from pyspark import SparkConf\n","sc = SparkContext()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RtUgrtwrt1Qa","colab_type":"text"},"source":["# Apache Spark 基本練習：\n","\n","是一個開源叢集運算框架，最初是由加州大學柏克萊分校AMPLab所開發。相對於Hadoop的MapReduce會在執行完工作後將中介資料存放到磁碟中，Spark使用了記憶體內運算技術，能在資料尚未寫入硬碟時即在記憶體內分析運算。Spark在記憶體內執行程式的運算速度能做到比Hadoop MapReduce的運算速度快上100倍。\n","\n","Some References :\n","\n","1. [http://www.mccarroll.net/blog/pyspark/index.html]\n","2. [https://www.codementor.io/spark/tutorial/spark-python-rdd-basics]\n","3. [http://backtobazics.com/big-data/spark/apache-spark-map-example/]\n","4. [http://datascience-enthusiast.com/Python/Apache_Spark1.html]\n","\n","## RDD 基本操作練習：\n","\n","\n","\n","### 產生一個整數隨機陣列： python語法："]},{"cell_type":"code","metadata":{"id":"I--b66Kzt1Qc","colab_type":"code","outputId":"4fcba3d4-e80b-4cd9-d090-bf32e309b919","executionInfo":{"status":"ok","timestamp":1571102576052,"user_tz":-480,"elapsed":27358,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":990}},"source":["import numpy as np\n","random_array = np.random.randint(1000, size=1000)\n","print (random_array)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[199 534 627 498 710 680 492 564 693 237 527 397 746 441  45 582 513 829\n"," 601 865 627  88 246 862 217 495  16 998 495 464 958 139 240 486  38 355\n"," 110 884 251 542 538 732 230 598 424 950 591 303 377 954   5 102 752 407\n"," 641 324 514 159 405 555  87  74  53 539 604 560 268 973 186 172 462 104\n","  90 450 374 926  56 789 265 202 851 488 342 671 158 104 428 204 737 563\n"," 388 210 327 491  55 126 705 672 477 346 799 860 642 495 467 741 220 661\n"," 275 597 758 772 965 633 127  85 207 994 859 375 467 230 296 862  11 445\n"," 455 999 553 114 493 592 300 977 105 877 586 705 416 288 285 874 429 913\n"," 650  29  15 428 974 930 348 651 672 970 449 627 908 159 685 450 500 571\n"," 709 138 512 104 152 921  13 165 391 846 366 207 226 692 626 659 528 683\n"," 612 661  60 995  99 234 608 296 888 385  26 914 711 245 409 720 255 228\n"," 931 550 514 928 393 391 966 386 722 267  54  57 644 962 875 753 907 563\n"," 933 938 470 730 534 193 882 814 717 425 781 710 548  38 617 281 906  39\n"," 467 267 112 248 334 143 576 539 602 741 205 733 585 884 844 946 636 928\n"," 719  45 514 872  88 151 551 312 170 642 643 171 341  81 448 576 330 667\n"," 895 267 594 257 102 225 427 345 869  55 508 643 827 834 363 340 775 144\n"," 457 510  72 919 160 364 578 455 911 305 993 933 698 480 517 400  24 673\n"," 273 725 378 222 529 802 986 262 294 101 689 609 385 464 432 553 824 507\n"," 885   8 423 486 338 583 448 528 962 457 370 984 921 102 808 801 435 901\n"," 558 353  95 354 980 578 577 834 305 887 890 686 228  30 307 218 866 980\n"," 447 561 264 883 752 407 673 575 967 991 847 491 838  31 808 623  37 101\n"," 329 496 227 159 931 129 335 701  27 861 571 735 116  46 496 153 882 608\n"," 533 502 222 681  56 927 986 676 660 795 788 339 961 867 459 666 762 614\n"," 627 812 389 251 644 449 449  49 682 466 445 216  80 595 282 845 490 394\n"," 133 263 882 202 563 205 979 872 846  81 857 288 991 668 705 702  44 740\n"," 320 250 515 580 887 118  21 723 491  95 199 106 932 697 205  99 983 337\n"," 615 796 310 169 723 911 274 133 991 348 848 810 690 643 344 199 142 810\n"," 392 489 613 630 672 616 280 385 235  87 672 655 101 181 293 727 825 181\n"," 460 214 201 638 412 833  74 378  89  93 123 235 933 517 190 474 158 834\n"," 204 732 774  49  94 207 719 233 661 268 405 492 812 178 476 868 990 946\n"," 814 957 607 830 856 435 430 237 253 596 550 956  91 181 301 279 816 532\n"," 351 283  20 613 349 637 245 604  77 489 740 196 707 906 732 885 200 372\n"," 476 711 321 361 239 124 254 555 546 751 315 943 669 569 227 460 927 682\n"," 723 427 964 943 876 819 788 847 409  37 645 706 667 871 256 372 352 675\n"," 572 941 742 849 963 151 247 128 630 325  93 874 643 203 684 944 471 528\n"," 805 294 295 255 123 729 849 885 455 153 875 213 617 469 702 465 838 398\n"," 159  59 872 623 565  41 409 462 361 804 912 501 680 386 392 493 544 423\n"," 586 256 642 760 780 903 895 245 332 704 392  27 146 172  68 144 288 298\n"," 857 699 575 713 135 752 968 820 292 671 189 243 158 728 559 222 185 264\n"," 917 521 428 835 276 254 711 966 825 303  43 322 255 901 497 242 467 942\n","  79 740 550 305   5 719 562 623 275 629 251 443 884 288 827 986 651 308\n"," 165 160 372 642 633 662 584 952 799 349 558 356 873 584 922 873 545 369\n"," 787  45 405 388 144 521 301 915 114 455 904 233 639 947 676 642 741 641\n"," 659 739 787 552  98 993 689  77 369 256 395 622 953 766 852  34 851 155\n"," 789 380 870 203 608  66 580 873 707 561 718 516 303 918 960  19 637 849\n","  63 446 650 767 791   2 543 895 277 440 637 933 240 516  58 638 744  22\n"," 352 188 217 605 638 138 161 849 398 241 190 910 631 250 451 696 818 787\n"," 584 493 316 691 850 971 355 215 395 321 478 714 890 563 245  78 683 940\n"," 615 187 879 652 442 149 141 121 768 365 792 354 621 704 964 111 831 442\n","  50 362 242 224 236 382 876 899 461 620 768 798 742 217 801 262 346 363\n"," 262 324 854 699 599 568 922 655 933  87 641 588 391 625 849 782 931 267\n"," 802  38 371  16 403 507 123 825 594 238 112 524 500 831 761 573 590 104\n","  46 707 484 184 223 387 758 932 709 732 500 992 171 184 377  15 180 912\n"," 529 898 481 929 145 274 567 846  16 319  25 763 197 205  55 967 930 750\n"," 166 947 606 166 486 918   6 967 906 860  91 538 688 778 859 495 608 736\n"," 189  44 219 741 276   8 400 820 261  51]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"l6qyKmz4t1Qg","colab_type":"text"},"source":["# 將資料轉成RDD，分別擺放於各spark executors上\n","\n","<img src=\"https://www.dropbox.com/s/br94ete5q3rj9w3/spark%20data%20model.png?dl=1\" width=\"500\" align=\"left\">\n","\n","\n","\n","\n","___\n","<img src=\"https://www.dropbox.com/s/l2gohpmn53jkv1b/%20spark%20system%20overview.png?dl=1\" width=\"500\" align=\"left\">"]},{"cell_type":"code","metadata":{"id":"MeFSq7rbt1Qg","colab_type":"code","colab":{}},"source":["rdd = sc.parallelize(random_array)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GiQqw6x0t1Qi","colab_type":"code","outputId":"6c481965-ceaa-4a4d-f626-07a842036eb6","executionInfo":{"status":"ok","timestamp":1571102576414,"user_tz":-480,"elapsed":27704,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["print(type(random_array))\n","print(type(rdd))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<class 'numpy.ndarray'>\n","<class 'pyspark.rdd.RDD'>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IMGznHMkt1Qk","colab_type":"text"},"source":["### RDD為Apache Spark最核心之概念，有別於MapRduce，僅提供Map()與Reduce()兩項操作。 RDD提供兩大類別Transformation與Action\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mJUQZL9lt1Qm","colab_type":"text"},"source":["> <img src=\"https://www.dropbox.com/s/omfoi3uzcgapcm4/rdd%20transformation%20concept.png?dl=1\" width=\"500\" align=\"left\">  "]},{"cell_type":"markdown","metadata":{"id":"kC-QgXwjt1Qm","colab_type":"text"},"source":["> <img src=\"https://www.dropbox.com/s/3u8gt5376qq5vjy/spark%20core.png?dl=1\" width=\"500\" align=\"left\">"]},{"cell_type":"markdown","metadata":{"id":"TQtTc14Wt1Qm","colab_type":"text"},"source":["## 最基本之Action操作  \n","### 使用 collect( ) 將分散於各機器之資料，收集成為單機資料集\n","<img src=\"https://www.dropbox.com/s/pjv20pl5wkevjf6/collect.png?dl=1\" width=\"500\" align=\"left\">\n"]},{"cell_type":"code","metadata":{"id":"11KfV3fPL6EG","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RnSzxgq9Nn0w","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZlZ-0sOSMXrx","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mN3hI6UAt1Qn","colab_type":"code","outputId":"f0989a5d-e2e6-44ac-f61f-417403f3c1d7","executionInfo":{"status":"ok","timestamp":1571102578034,"user_tz":-480,"elapsed":29302,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["rdd.first()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["199"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"fa8Z0Zwht1Qp","colab_type":"text"},"source":["## Transformation 觀念介紹"]},{"cell_type":"code","metadata":{"id":"XFThLRfCt1Qq","colab_type":"code","outputId":"56dfdb77-ac40-427b-f5d2-ca43329ef933","executionInfo":{"status":"ok","timestamp":1571102578035,"user_tz":-480,"elapsed":29297,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["def doubling(x):\n","    return x*2\n","\n","print(doubling(10))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["20\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cVLdPhDNt1Qs","colab_type":"code","outputId":"d64d31b7-b69c-4b1d-f78b-ddce0eaa529e","executionInfo":{"status":"ok","timestamp":1571102578037,"user_tz":-480,"elapsed":29294,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["rdd.take(5)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[199, 534, 627, 498, 710]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"FqAxyXPqt1Qu","colab_type":"code","outputId":"c3f51532-5362-4657-c815-6a256ac82c72","executionInfo":{"status":"ok","timestamp":1571102578037,"user_tz":-480,"elapsed":29287,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["rdd.map(doubling).take(5)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[398, 1068, 1254, 996, 1420]"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"0FQiZbX3wFvf","colab_type":"code","colab":{}},"source":["def minusone(y):\n","  return y-1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tGwRUarGwVio","colab_type":"code","outputId":"f577d375-e9df-475a-f9a5-68b1a63fcf78","executionInfo":{"status":"ok","timestamp":1571102578039,"user_tz":-480,"elapsed":29279,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["rdd.map(minusone).take(5)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[198, 533, 626, 497, 709]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"PCP1XQ-mt1Qx","colab_type":"text"},"source":["#### 注意：匿名函式的使用  \n","    rdd.map(doubling) = rdd.map(lambda x: x*2)"]},{"cell_type":"code","metadata":{"id":"F00w0ISRhPck","colab_type":"code","outputId":"81e8ce2f-3628-4c60-e8a9-b48eab193ffc","executionInfo":{"status":"ok","timestamp":1571102578389,"user_tz":-480,"elapsed":29624,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["rdd.map(lambda x: x-1).take(5)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[198, 533, 626, 497, 709]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"paFPu3J9yUng","colab_type":"code","outputId":"5791f913-1b4a-4a58-8eca-462e0585f45c","executionInfo":{"status":"ok","timestamp":1571102578389,"user_tz":-480,"elapsed":29620,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["rdd.map(lambda yy:0).take(2)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 0]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"4Anc1mSLzEmp","colab_type":"code","outputId":"870d2a54-02fe-4c92-9da3-89b1a95b4e2b","executionInfo":{"status":"ok","timestamp":1571102578392,"user_tz":-480,"elapsed":29618,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["rdd.map(lambda x: x-1).map(lambda x:x-2).map(lambda x:x-3).take(2)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[193, 528]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"iJeJB9tNt1Qx","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3rfUg7nrt1Q0","colab_type":"text"},"source":["#### 練習：使用map()，將所有數字開平方根 \n","    import math\n","    print math.sqrt(5)"]},{"cell_type":"code","metadata":{"id":"injnEU2Lt1Q1","colab_type":"code","outputId":"4c93452b-f570-481c-a3d9-0e2a749f6037","executionInfo":{"status":"ok","timestamp":1571102578765,"user_tz":-480,"elapsed":29982,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import math\n","rdd.map(lambda x: math.sqrt(x)).take(2)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[14.106735979665885, 23.108440016582687]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"n7PtJhL1t1Q4","colab_type":"text"},"source":["#### 匿名函式的差異"]},{"cell_type":"code","metadata":{"id":"LTFiSaMdt1Q5","colab_type":"code","colab":{}},"source":["import math\n","def sqrt(x):\n","    return math.sqrt(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Cz3ZPi1iL06","colab_type":"code","outputId":"61659b1d-7222-4605-fe65-17734ab516cb","executionInfo":{"status":"ok","timestamp":1571102578766,"user_tz":-480,"elapsed":29975,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["rdd.map(lambda y: math.sqrt(y)).take(2)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[14.106735979665885, 23.108440016582687]"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"zCnE4mS_iJ1m","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"A75twst-t1Q7","colab_type":"code","outputId":"1f3e6f7e-6acc-4a40-b7a0-07b8e7e19467","executionInfo":{"status":"ok","timestamp":1571102578767,"user_tz":-480,"elapsed":29972,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["rdd.map(sqrt).take(2)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[14.106735979665885, 23.108440016582687]"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"olf5dYNO1JP8","colab_type":"text"},"source":["# Transformation Operators: map(), filter(), sample(), groupBy(), etc \n","## (完整Transformation Operator請參考下列網址) http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD "]},{"cell_type":"markdown","metadata":{"id":"m-dnb6Vb-RnK","colab_type":"text"},"source":["rdd資料中有多少筆?"]},{"cell_type":"code","metadata":{"id":"qBhcOOPJ9sUa","colab_type":"code","outputId":"c0d845ad-0657-4bd9-a10e-061ef4ff622d","executionInfo":{"status":"ok","timestamp":1571102579112,"user_tz":-480,"elapsed":30311,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["rdd.map(lambda x:1).reduce(lambda x,y:x+y) "],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1000"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"952iKzm8-QqL","colab_type":"code","outputId":"219fe343-869d-457c-ecc7-4db3cfdcc9d4","executionInfo":{"status":"ok","timestamp":1571102579113,"user_tz":-480,"elapsed":30306,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["rdd.count()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1000"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"HMYWs-X-1Usl","colab_type":"code","outputId":"b8e913fb-1bd2-4c16-ff4a-be2452229d4c","executionInfo":{"status":"ok","timestamp":1571102579115,"user_tz":-480,"elapsed":30303,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["rdd.map(lambda x: x if x%2==0 else 0).reduce(lambda x,y:x+y)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["251504"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"6uYc89h1FE1e","colab_type":"code","outputId":"e2f12f11-1e44-4dac-f713-dd0f7263d7d4","executionInfo":{"status":"ok","timestamp":1571103794308,"user_tz":-480,"elapsed":839,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["rdd.map(lambda x: 1 if x%2==1 else 0).reduce(lambda x,y:x+y) # odd"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["512"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"code","metadata":{"id":"i6U0rLWaFY10","colab_type":"code","outputId":"a9833502-f0ad-42a4-fdd6-3b86a35b7446","executionInfo":{"status":"ok","timestamp":1571103819323,"user_tz":-480,"elapsed":831,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["rdd.map(lambda x: 1 if x%2==0 else 0).reduce(lambda x,y:x+y) # even"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["488"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"code","metadata":{"id":"4Xl0XyJkIaCF","colab_type":"code","outputId":"e01b84de-d42e-4ac9-ead9-61ec6db3642d","executionInfo":{"status":"ok","timestamp":1571105802823,"user_tz":-480,"elapsed":1417,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["rdd.map(lambda x: ('even', 1) if x%2==0 else ('odd',1))\\\n","   .reduceByKey(lambda x,y: x+y)\\\n","   .take(2)\n","\n","#keyValueRdd = rdd.map(lambda x: ('even', 1) if x%2==0 else ('odd',1))\n","\n","#keyValueRdd.reduceByKey(lambda x,y: x+y).take(2)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('even', 488), ('odd', 512)]"]},"metadata":{"tags":[]},"execution_count":67}]},{"cell_type":"markdown","metadata":{"id":"elsAvKXNt1Q-","colab_type":"text"},"source":["#### Transformation Operator 使用filter()，將所有偶數留下，奇數刪除。"]},{"cell_type":"code","metadata":{"id":"raH2ldfBt1Q-","colab_type":"code","outputId":"aa45fc47-897a-47f2-bd6f-40051bb5b826","executionInfo":{"status":"ok","timestamp":1571102579457,"user_tz":-480,"elapsed":30640,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["rdd.filter(lambda x: x%2==0).reduce(lambda x,y:x+y)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["251504"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"uTLr4oJEt1RB","colab_type":"text"},"source":["#### Transformation Operator 使用sample( ) 抽樣給定比例之RDD子集合"]},{"cell_type":"code","metadata":{"id":"n7U8Q8G-t1RB","colab_type":"code","outputId":"7d29f2bc-3fc2-4a4b-dfd8-67bd72a3e9e5","executionInfo":{"status":"ok","timestamp":1571102579459,"user_tz":-480,"elapsed":30637,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["subsetrdd = rdd.sample(False, 0.01)\n","subsetrdd.collect()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[829, 110, 56, 477, 205, 471, 890, 798, 524, 387]"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"8jpIFRx3t1RE","colab_type":"text"},"source":["#### Transformation Operator: 使用 groupBy( ) 將資料分組"]},{"cell_type":"code","metadata":{"id":"jffxeQevt1RE","colab_type":"code","outputId":"cfe2c400-0e29-4cf2-ac12-81614841f59c","executionInfo":{"status":"ok","timestamp":1571102580323,"user_tz":-480,"elapsed":31496,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["result = subsetrdd.groupBy(lambda x: x%2==0).collect()\n","print(result)\n","print([(x, list(y)) for (x, y) in result])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[(False, <pyspark.resultiterable.ResultIterable object at 0x7f9113cbf710>), (True, <pyspark.resultiterable.ResultIterable object at 0x7f9113e6f9e8>)]\n","[(False, [829, 477, 205, 471, 387]), (True, [110, 56, 890, 798, 524])]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fmcD3Rb41HDj","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dhVPws5qt1RG","colab_type":"text"},"source":["#### Transformation Operator 使用map ( ) 產生 key value pair"]},{"cell_type":"code","metadata":{"id":"-eo0RYkqt1RH","colab_type":"code","colab":{}},"source":["keyValueRdd = rdd.map(lambda x: ('even', x) if x%2==0 else ('odd',x))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v7s_mWe6_-ab","colab_type":"code","outputId":"03aa7c0b-388d-41af-dd18-2a1a8a590daa","executionInfo":{"status":"ok","timestamp":1571106764613,"user_tz":-480,"elapsed":903,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["rdd.map(lambda x: ('even', x) if x%2==0 else ('odd',x))\\\n","   .reduceByKey(lambda x,y: x+y)\\\n","   .take(2)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('even', 251504), ('odd', 255154)]"]},"metadata":{"tags":[]},"execution_count":74}]},{"cell_type":"code","metadata":{"id":"KjpBTvuY6kJr","colab_type":"code","colab":{}},"source":["def evennumber(x):\n","  if x%2==0:\n","    return x\n","  else:\n","    return 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WP-RPBA36wYD","colab_type":"code","outputId":"61847881-a59d-407c-9862-920bfe43810d","executionInfo":{"status":"ok","timestamp":1571102580712,"user_tz":-480,"elapsed":31870,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["evennumber(101)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"w5hM0Gow7WOl","colab_type":"code","outputId":"a01166b0-e13c-4af9-f4c1-39b544978ce9","executionInfo":{"status":"ok","timestamp":1571102580715,"user_tz":-480,"elapsed":31869,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["rdd.reduce(lambda x,y: x*x+y*y)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in long_scalars\n","  \"\"\"Entry point for launching an IPython kernel.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["3545913053573636722"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"TBWfbG8S60vF","colab_type":"code","outputId":"32667c84-c679-4b97-af65-b748e4b8845f","executionInfo":{"status":"ok","timestamp":1571102580715,"user_tz":-480,"elapsed":31864,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["rdd.map(evennumber).reduce(lambda x,y:x+y)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["251504"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"ipn1B9MJ3xND","colab_type":"code","outputId":"4efc6722-2754-49b7-b97e-3a5e1042c4a0","executionInfo":{"status":"ok","timestamp":1571102581072,"user_tz":-480,"elapsed":32216,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["rdd.map(lambda x: x if x%2==0 else 0).reduce(lambda x,y:x+y)\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["251504"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"ergjRAqbt1RI","colab_type":"text"},"source":["#### Transformation Operator: 使用 groupbyKey ( ) 根據key將資料分組"]},{"cell_type":"code","metadata":{"id":"0lU_2LrIt1RJ","colab_type":"code","outputId":"166ded29-33f6-466a-d92d-df60539872b6","executionInfo":{"status":"ok","timestamp":1571102581073,"user_tz":-480,"elapsed":32209,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["\n","keyValueRdd = rdd.map(lambda x: ('even', x) if x%2==0 else ('odd',x))\n","\n","for x in keyValueRdd.groupByKey().collect():\n","    print (x[0], list(x[1]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["even [534, 498, 710, 680, 492, 564, 746, 582, 88, 246, 862, 16, 998, 464, 958, 240, 486, 38, 110, 884, 542, 538, 732, 230, 598, 424, 950, 954, 102, 752, 324, 514, 74, 604, 560, 268, 186, 172, 462, 104, 90, 450, 374, 926, 56, 202, 488, 342, 158, 104, 428, 204, 388, 210, 126, 672, 346, 860, 642, 220, 758, 772, 994, 230, 296, 862, 114, 592, 300, 586, 416, 288, 874, 650, 428, 974, 930, 348, 672, 970, 908, 450, 500, 138, 512, 104, 152, 846, 366, 226, 692, 626, 528, 612, 60, 234, 608, 296, 888, 26, 914, 720, 228, 550, 514, 928, 966, 386, 722, 54, 644, 962, 938, 470, 730, 534, 882, 814, 710, 548, 38, 906, 112, 248, 334, 576, 602, 884, 844, 946, 636, 928, 514, 872, 88, 312, 170, 642, 448, 576, 330, 594, 102, 508, 834, 340, 144, 510, 72, 160, 364, 578, 698, 480, 400, 24, 378, 222, 802, 986, 262, 294, 464, 432, 824, 8, 486, 338, 448, 528, 962, 370, 984, 102, 808, 558, 354, 980, 578, 834, 890, 686, 228, 30, 218, 866, 980, 264, 752, 838, 808, 496, 116, 46, 496, 882, 608, 502, 222, 56, 986, 676, 660, 788, 666, 762, 614, 812, 644, 682, 466, 216, 80, 282, 490, 394, 882, 202, 872, 846, 288, 668, 702, 44, 740, 320, 250, 580, 118, 106, 932, 796, 310, 274, 348, 848, 810, 690, 344, 142, 810, 392, 630, 672, 616, 280, 672, 460, 214, 638, 412, 74, 378, 190, 474, 158, 834, 204, 732, 774, 94, 268, 492, 812, 178, 476, 868, 990, 946, 814, 830, 856, 430, 596, 550, 956, 816, 532, 20, 604, 740, 196, 906, 732, 200, 372, 476, 124, 254, 546, 460, 682, 964, 876, 788, 706, 256, 372, 352, 572, 742, 128, 630, 874, 684, 944, 528, 294, 702, 838, 398, 872, 462, 804, 912, 680, 386, 392, 544, 586, 256, 642, 760, 780, 332, 704, 392, 146, 172, 68, 144, 288, 298, 752, 968, 820, 292, 158, 728, 222, 264, 428, 276, 254, 966, 322, 242, 942, 740, 550, 562, 884, 288, 986, 308, 160, 372, 642, 662, 584, 952, 558, 356, 584, 922, 388, 144, 114, 904, 676, 642, 552, 98, 256, 622, 766, 852, 34, 380, 870, 608, 66, 580, 718, 516, 918, 960, 446, 650, 2, 440, 240, 516, 58, 638, 744, 22, 352, 188, 638, 138, 398, 190, 910, 250, 696, 818, 584, 316, 850, 478, 714, 890, 78, 940, 652, 442, 768, 792, 354, 704, 964, 442, 50, 362, 242, 224, 236, 382, 876, 620, 768, 798, 742, 262, 346, 262, 324, 854, 568, 922, 588, 782, 802, 38, 16, 594, 238, 112, 524, 500, 590, 104, 46, 484, 184, 758, 932, 732, 500, 992, 184, 180, 912, 898, 274, 846, 16, 930, 750, 166, 606, 166, 486, 918, 6, 906, 860, 538, 688, 778, 608, 736, 44, 276, 8, 400, 820]\n","odd [199, 627, 693, 237, 527, 397, 441, 45, 513, 829, 601, 865, 627, 217, 495, 495, 139, 355, 251, 591, 303, 377, 5, 407, 641, 159, 405, 555, 87, 53, 539, 973, 789, 265, 851, 671, 737, 563, 327, 491, 55, 705, 477, 799, 495, 467, 741, 661, 275, 597, 965, 633, 127, 85, 207, 859, 375, 467, 11, 445, 455, 999, 553, 493, 977, 105, 877, 705, 285, 429, 913, 29, 15, 651, 449, 627, 159, 685, 571, 709, 921, 13, 165, 391, 207, 659, 683, 661, 995, 99, 385, 711, 245, 409, 255, 931, 393, 391, 267, 57, 875, 753, 907, 563, 933, 193, 717, 425, 781, 617, 281, 39, 467, 267, 143, 539, 741, 205, 733, 585, 719, 45, 151, 551, 643, 171, 341, 81, 667, 895, 267, 257, 225, 427, 345, 869, 55, 643, 827, 363, 775, 457, 919, 455, 911, 305, 993, 933, 517, 673, 273, 725, 529, 101, 689, 609, 385, 553, 507, 885, 423, 583, 457, 921, 801, 435, 901, 353, 95, 577, 305, 887, 307, 447, 561, 883, 407, 673, 575, 967, 991, 847, 491, 31, 623, 37, 101, 329, 227, 159, 931, 129, 335, 701, 27, 861, 571, 735, 153, 533, 681, 927, 795, 339, 961, 867, 459, 627, 389, 251, 449, 449, 49, 445, 595, 845, 133, 263, 563, 205, 979, 81, 857, 991, 705, 515, 887, 21, 723, 491, 95, 199, 697, 205, 99, 983, 337, 615, 169, 723, 911, 133, 991, 643, 199, 489, 613, 385, 235, 87, 655, 101, 181, 293, 727, 825, 181, 201, 833, 89, 93, 123, 235, 933, 517, 49, 207, 719, 233, 661, 405, 957, 607, 435, 237, 253, 91, 181, 301, 279, 351, 283, 613, 349, 637, 245, 77, 489, 707, 885, 711, 321, 361, 239, 555, 751, 315, 943, 669, 569, 227, 927, 723, 427, 943, 819, 847, 409, 37, 645, 667, 871, 675, 941, 849, 963, 151, 247, 325, 93, 643, 203, 471, 805, 295, 255, 123, 729, 849, 885, 455, 153, 875, 213, 617, 469, 465, 159, 59, 623, 565, 41, 409, 361, 501, 493, 423, 903, 895, 245, 27, 857, 699, 575, 713, 135, 671, 189, 243, 559, 185, 917, 521, 835, 711, 825, 303, 43, 255, 901, 497, 467, 79, 305, 5, 719, 623, 275, 629, 251, 443, 827, 651, 165, 633, 799, 349, 873, 873, 545, 369, 787, 45, 405, 521, 301, 915, 455, 233, 639, 947, 741, 641, 659, 739, 787, 993, 689, 77, 369, 395, 953, 851, 155, 789, 203, 873, 707, 561, 303, 19, 637, 849, 63, 767, 791, 543, 895, 277, 637, 933, 217, 605, 161, 849, 241, 631, 451, 787, 493, 691, 971, 355, 215, 395, 321, 563, 245, 683, 615, 187, 879, 149, 141, 121, 365, 621, 111, 831, 899, 461, 217, 801, 363, 699, 599, 655, 933, 87, 641, 391, 625, 849, 931, 267, 371, 403, 507, 123, 825, 831, 761, 573, 707, 223, 387, 709, 171, 377, 15, 529, 481, 929, 145, 567, 319, 25, 763, 197, 205, 55, 967, 947, 967, 91, 859, 495, 189, 219, 741, 261, 51]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2cF4eO2ht1RL","colab_type":"text"},"source":["\n","###練習 計算奇數加總值與偶數加總值\n","#### groupbyKey(), reduceByKey()"]},{"cell_type":"code","metadata":{"id":"JvI54vHQt1RL","colab_type":"code","outputId":"66233568-e20a-4243-c0bf-608291513f98","executionInfo":{"status":"ok","timestamp":1571102581413,"user_tz":-480,"elapsed":32545,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["keyValueRdd.reduceByKey(lambda x,y: x+y).take(2)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('even', 251504), ('odd', 255154)]"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"2zt-atriGcY1","colab_type":"code","outputId":"7337c221-b8e3-4043-a321-acef1f320016","executionInfo":{"status":"ok","timestamp":1571102581762,"user_tz":-480,"elapsed":32890,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["rdd.map(lambda x: (x,1)).reduceByKey(lambda a,b:a+b).take(5)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(534, 2), (498, 1), (710, 2), (680, 2), (492, 2)]"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"GeI84meUt1Re","colab_type":"text"},"source":["___\n","___\n","___\n","\n","\n","# RDD Action Action Operation\n","#### 使用reduce()，計算所有數之加總值。"]},{"cell_type":"code","metadata":{"id":"YML9Ihvet1Re","colab_type":"code","outputId":"715a41ee-adaa-4fa5-9548-5a17de671acb","executionInfo":{"status":"ok","timestamp":1571102582226,"user_tz":-480,"elapsed":33349,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["rdd.reduce(lambda x,y: x+y )"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["506658"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"K7VhWlCUfIrn","colab_type":"code","outputId":"0078b45d-0e96-4583-9654-186068890a2f","executionInfo":{"status":"ok","timestamp":1571102582227,"user_tz":-480,"elapsed":33345,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["rdd.reduce(lambda x,y: x*x+y*y)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in long_scalars\n","  \"\"\"Entry point for launching an IPython kernel.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["3545913053573636722"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"GYMr5QHb0Isc","colab_type":"code","outputId":"ad612855-b7ba-49e1-a724-6c7bfe22af89","executionInfo":{"status":"ok","timestamp":1571102582228,"user_tz":-480,"elapsed":33342,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["rdd.map(lambda x:1).reduce(lambda a,b:a+b)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1000"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"vjiNSGoD0p1r","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OQWCsi3Nt1Rq","colab_type":"text"},"source":["______\n","\n","## 觀念\n","### 練習：計算所有數字平方和。"]},{"cell_type":"code","metadata":{"id":"4cFvuXYX2iA7","colab_type":"code","outputId":"0e0638d2-40d6-4ea1-86d8-028102f4f3fa","executionInfo":{"status":"ok","timestamp":1571102582229,"user_tz":-480,"elapsed":33335,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["rdd.map(lambda x: x*x).reduce(lambda x,y: x+y)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["337043784"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"markdown","metadata":{"id":"6rJDXbsZt1Rr","colab_type":"text"},"source":["## 練習：使用map(), reduceByKey()，計算所有數字出現頻率。"]},{"cell_type":"code","metadata":{"id":"z13lFWqgt1Rr","colab_type":"code","outputId":"28e7755d-3a0b-4a4f-ab43-63cee3e25171","executionInfo":{"status":"ok","timestamp":1571102582571,"user_tz":-480,"elapsed":33672,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["rdd.map(lambda x: (x,1)).reduceByKey(lambda a,b:a+b)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PythonRDD[54] at RDD at PythonRDD.scala:48"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"0N__bRU5t1Rv","colab_type":"text"},"source":["_____\n","_____\n","_____\n","\n","# WordCount"]},{"cell_type":"code","metadata":{"id":"R_cMDKMet1Rx","colab_type":"code","outputId":"42f988c7-88f1-474c-f8ba-ecbc6f993751","executionInfo":{"status":"ok","timestamp":1571102586108,"user_tz":-480,"elapsed":37204,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":384}},"source":["!wget -O speech \"https://www.dropbox.com/s/28ljfwb1aeuyi37/speech.txt?dl=1\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2019-10-15 01:23:22--  https://www.dropbox.com/s/28ljfwb1aeuyi37/speech.txt?dl=1\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.82.1, 2620:100:6032:1::a27d:5201\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.82.1|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/dl/28ljfwb1aeuyi37/speech.txt [following]\n","--2019-10-15 01:23:22--  https://www.dropbox.com/s/dl/28ljfwb1aeuyi37/speech.txt\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc936573473e8f69fb784b70bc55.dl.dropboxusercontent.com/cd/0/get/AqfSQ70ZMZbpqwsXQm3xzzgxDS8ODojconBHB5o1kMk2cPKOhcxa7H3e-ZBEpwZZBHqJ_kV0UJk2aL0XYnH953BALDhLcF1bwit3DvCCLwwIag/file?dl=1# [following]\n","--2019-10-15 01:23:22--  https://uc936573473e8f69fb784b70bc55.dl.dropboxusercontent.com/cd/0/get/AqfSQ70ZMZbpqwsXQm3xzzgxDS8ODojconBHB5o1kMk2cPKOhcxa7H3e-ZBEpwZZBHqJ_kV0UJk2aL0XYnH953BALDhLcF1bwit3DvCCLwwIag/file?dl=1\n","Resolving uc936573473e8f69fb784b70bc55.dl.dropboxusercontent.com (uc936573473e8f69fb784b70bc55.dl.dropboxusercontent.com)... 162.125.82.6, 2620:100:6032:6::a27d:5206\n","Connecting to uc936573473e8f69fb784b70bc55.dl.dropboxusercontent.com (uc936573473e8f69fb784b70bc55.dl.dropboxusercontent.com)|162.125.82.6|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 21601 (21K) [application/binary]\n","Saving to: ‘speech’\n","\n","speech              100%[===================>]  21.09K  --.-KB/s    in 0.01s   \n","\n","2019-10-15 01:23:23 (1.52 MB/s) - ‘speech’ saved [21601/21601]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xsHULr-xt1Ry","colab_type":"code","colab":{}},"source":["data_file = \"./speech\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qiBihzxpakdB","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wmkX3FKlt1R0","colab_type":"code","colab":{}},"source":["text_file = sc.textFile(data_file)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g1cLYnV1OF_T","colab_type":"code","outputId":"4d0d97d4-8ecb-489c-edfb-4a4e242b5aff","executionInfo":{"status":"ok","timestamp":1571106096869,"user_tz":-480,"elapsed":907,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["text_file.take(5)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['各位 友邦 的 元首 與 貴賓 、 各國 駐台 使節 及 代表 、 現場 的 好 朋友 ， 全體 國人 同胞 ， 大家 好 。   ',\n"," '感謝 與 承擔   就 在 剛剛 ， 我 和 陳建仁 已經 在 總統府 裡面 ， 正式 宣誓 就任 中華民國 第十四 任 ',\n"," '總統 與 副 總統 。 我們 要 感謝 這塊 土地 對 我們 的 栽培 ， 感謝 人民 對 我們 的 信任 ， ',\n"," '以及 ， 最 重要 的 ， 感謝 這個 國家 的 民主 機制 ， 讓 我們 透過 和平 的 選舉 過程 ， 實現 第三次 政黨 輪替 ，',\n"," '並且 克服 種種 不 確定 因素 ， 順利 渡過 長達 四個 月 的 交接 期 ， 完成 政權 和平 移轉 。   台灣 ， ']"]},"metadata":{"tags":[]},"execution_count":68}]},{"cell_type":"code","metadata":{"id":"SzhZOO8B54_d","colab_type":"code","outputId":"c1ada29e-2be6-4c6d-a7d4-387e73ea30d3","executionInfo":{"status":"ok","timestamp":1571102586486,"user_tz":-480,"elapsed":37571,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(type(text_file))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<class 'pyspark.rdd.RDD'>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ucnc2gbbdKuG","colab_type":"code","outputId":"24ca605e-bca9-409d-9ee0-4488b9881409","executionInfo":{"status":"ok","timestamp":1571106721788,"user_tz":-480,"elapsed":911,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["text_file.map(lambda line: line.split(\" \")).count()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["89"]},"metadata":{"tags":[]},"execution_count":72}]},{"cell_type":"code","metadata":{"id":"JlZxjR_sda8D","colab_type":"code","outputId":"eb3c873e-3dfa-499d-cddc-a9b572475bb1","executionInfo":{"status":"ok","timestamp":1571106720108,"user_tz":-480,"elapsed":885,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["text_file.flatMap(lambda line: line.split(\" \")).count()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3776"]},"metadata":{"tags":[]},"execution_count":71}]},{"cell_type":"code","metadata":{"id":"NAIpNQIFt1R2","colab_type":"code","outputId":"f3d70d56-ac26-4e77-9df0-761b06b3dbe4","executionInfo":{"status":"ok","timestamp":1571102586488,"user_tz":-480,"elapsed":37560,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":191}},"source":["wordcountsRDD = text_file.flatMap(lambda line: line.split(\" \")) \\\n","             .map(lambda word: (word, 1)) \\\n","             .reduceByKey(lambda a, b: a + b)\n","wordcountsRDD.take(10)\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('各位', 3),\n"," ('友邦', 2),\n"," ('的', 292),\n"," ('元首', 2),\n"," ('與', 37),\n"," ('各國', 3),\n"," ('駐台', 1),\n"," ('使節', 1),\n"," ('及', 14),\n"," ('現場', 2)]"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"code","metadata":{"id":"sqSJXuJ0Qwka","colab_type":"code","outputId":"d782c00b-9c48-4f22-88c1-b7528e2aaaa2","executionInfo":{"status":"ok","timestamp":1571102586488,"user_tz":-480,"elapsed":37554,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":191}},"source":["wordcountsRDD.take(10)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('各位', 3),\n"," ('友邦', 2),\n"," ('的', 292),\n"," ('元首', 2),\n"," ('與', 37),\n"," ('各國', 3),\n"," ('駐台', 1),\n"," ('使節', 1),\n"," ('及', 14),\n"," ('現場', 2)]"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"markdown","metadata":{"id":"kLRha3Eet1R4","colab_type":"text"},"source":["#### 根據字元符號順序做排序"]},{"cell_type":"code","metadata":{"id":"rngFUG_0t1R6","colab_type":"code","outputId":"16ea1808-c985-4e64-fc3e-a56158fb75a4","executionInfo":{"status":"ok","timestamp":1571102587361,"user_tz":-480,"elapsed":38424,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["text_file = sc.textFile(data_file)\n","counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n","             .map(lambda word: (word, 1)) \\\n","             .reduceByKey(lambda a, b: a + b).sortByKey()\n","counts.take(5)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('', 228), ('1992', 2), ('1996', 1), ('20', 4), ('2016', 1)]"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"markdown","metadata":{"id":"62tc3XbHt1R7","colab_type":"text"},"source":["#### 出現頻率做排序"]},{"cell_type":"code","metadata":{"id":"KeUmGmNJt1R8","colab_type":"code","outputId":"4ed0cab6-4271-48d6-9b88-bcbf77d2d1d3","executionInfo":{"status":"ok","timestamp":1571107132494,"user_tz":-480,"elapsed":1228,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":364}},"source":["text_file = sc.textFile(data_file)\n","\n","counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n","             .map(lambda word: (word, 1)) \\\n","             .reduceByKey(lambda a, b: a + b)\\\n","             .sortBy(lambda x: x[1], ascending=False)\n","\n","counts.take(20)\n","\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('，', 336),\n"," ('的', 292),\n"," ('', 228),\n"," ('。', 159),\n"," ('我們', 86),\n"," ('、', 59),\n"," ('台灣', 39),\n"," ('與', 37),\n"," ('在', 33),\n"," ('國家', 32),\n"," ('和', 31),\n"," ('一個', 29),\n"," ('新政府', 27),\n"," ('是', 27),\n"," ('要', 25),\n"," ('經濟', 25),\n"," ('這個', 25),\n"," ('讓', 24),\n"," ('會', 24),\n"," ('也', 22)]"]},"metadata":{"tags":[]},"execution_count":78}]},{"cell_type":"code","metadata":{"id":"ACaLE4I4TaL0","colab_type":"code","outputId":"eb25e0ad-e06b-4883-c31b-2470f4b7cd73","executionInfo":{"status":"ok","timestamp":1571107728138,"user_tz":-480,"elapsed":1252,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":364}},"source":["text_file = sc.textFile(data_file)\n","\n","counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n","             .map(lambda word: (word, 1)) \\\n","             .reduceByKey(lambda a, b: a + b)\\\n","             .map(lambda x: (x[1],x[0]))\\\n","             .sortByKey(False)\n","\n","counts.take(20)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(336, '，'),\n"," (292, '的'),\n"," (228, ''),\n"," (159, '。'),\n"," (86, '我們'),\n"," (59, '、'),\n"," (39, '台灣'),\n"," (37, '與'),\n"," (33, '在'),\n"," (32, '國家'),\n"," (31, '和'),\n"," (29, '一個'),\n"," (27, '新政府'),\n"," (27, '是'),\n"," (25, '要'),\n"," (25, '經濟'),\n"," (25, '這個'),\n"," (24, '讓'),\n"," (24, '會'),\n"," (22, '也')]"]},"metadata":{"tags":[]},"execution_count":80}]},{"cell_type":"markdown","metadata":{"id":"sQasOGYAt1R_","colab_type":"text"},"source":["#### map 與 flatMap的差異"]},{"cell_type":"code","metadata":{"id":"TKZtTRUXt1R_","colab_type":"code","outputId":"ecfe97b2-ee1e-4ad8-a555-6569d0390779","executionInfo":{"status":"ok","timestamp":1571102588053,"user_tz":-480,"elapsed":39107,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":503}},"source":["text_file = sc.textFile(data_file)\n","counts = text_file.map(lambda line: line.split(\" \"))\n","counts.take(1)\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['各位',\n","  '友邦',\n","  '的',\n","  '元首',\n","  '與',\n","  '貴賓',\n","  '、',\n","  '各國',\n","  '駐台',\n","  '使節',\n","  '及',\n","  '代表',\n","  '、',\n","  '現場',\n","  '的',\n","  '好',\n","  '朋友',\n","  '，',\n","  '全體',\n","  '國人',\n","  '同胞',\n","  '，',\n","  '大家',\n","  '好',\n","  '。',\n","  '',\n","  '',\n","  '']]"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"code","metadata":{"id":"tDvjL-8Tt1SC","colab_type":"code","outputId":"82feb68b-3346-45e4-b1a9-80b40d49f13e","executionInfo":{"status":"ok","timestamp":1571102588054,"user_tz":-480,"elapsed":39103,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["text_file = sc.textFile(data_file)\n","counts = text_file.flatMap(lambda line: line.split(\" \"))\n","counts.take(1)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['各位']"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"code","metadata":{"id":"sKvDFz9rKk94","colab_type":"code","outputId":"7db75d9b-fa16-4763-f0ed-b504f689573c","executionInfo":{"status":"ok","timestamp":1571102588408,"user_tz":-480,"elapsed":39453,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["rdd.map(lambda x: x if x>500 else 0).reduce(lambda x,y:x+y)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["379374"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"markdown","metadata":{"id":"QVX1K9rvt1SE","colab_type":"text"},"source":["\n","____\n","____\n","____\n","# pi-estimation "]},{"cell_type":"code","metadata":{"id":"7qJCgsskt1SE","colab_type":"code","outputId":"bb3459f5-2ceb-420b-c46c-4fa269d70d0c","executionInfo":{"status":"ok","timestamp":1571102593604,"user_tz":-480,"elapsed":44646,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import random\n","\n","def sample(p):\n","    x, y = random.random(), random.random()\n","    return 1 if x*x + y*y < 1 else 0\n","\n","count = sc.parallelize(range(0, 10000000)).map(sample) \\\n","             .reduce(lambda a, b: a + b)\n","print (\"Pi is roughly %f\" % (4.0 * count / 10000000))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Pi is roughly 3.140703\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OAQNfVent1SF","colab_type":"code","outputId":"582304d2-177e-4d5e-fe31-5f4be63e86aa","executionInfo":{"status":"ok","timestamp":1571102593950,"user_tz":-480,"elapsed":44987,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["count = sc.parallelize(range(0, 1000000))\\\n","        .map(lambda p: 1 if (random.random()**2 + random.random()**2)<1 else 0) \\\n","        .reduce(lambda a, b: a + b)\n","print (\"Pi is roughly %f\" % (4.0 * count / 1000000))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Pi is roughly 3.137376\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WXI8RPNEt1SH","colab_type":"text"},"source":["___\n","___\n","___\n","\n","# Text Search Example"]},{"cell_type":"code","metadata":{"id":"ym_ec0yDt1SI","colab_type":"code","outputId":"b6228baf-9a28-40b8-94b6-248a231466e8","executionInfo":{"status":"error","timestamp":1571102594805,"user_tz":-480,"elapsed":45839,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":237}},"source":["import urllib\n","f=urllib.urlretrieve(\"https://www.ccel.org/ccel/bible/kjv.txt\",\"bible\")\n","text_file = sc.textFile(data_file)\n","lines = text_file.map(lambda line: line) \n","lines.take(20)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-58-ba0407fb40a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.ccel.org/ccel/bible/kjv.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"bible\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtext_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'urllib' has no attribute 'urlretrieve'"]}]},{"cell_type":"markdown","metadata":{"id":"W8dnzpSUt1SJ","colab_type":"text"},"source":["\n","___\n","___\n","___\n","\n","# Filter Example"]},{"cell_type":"code","metadata":{"id":"aVpHViqwt1SK","colab_type":"code","colab":{}},"source":["import urllib\n","urllib.urlretrieve(\"https://www.ccel.org/ccel/bible/kjv.txt\",\"bible\")\n","data_file = \"./bible\"\n","text_file = sc.textFile(data_file)\n","lines = text_file.filter(lambda line: 'and' in line) \n","lines.take(20)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_fErZ1SNt1SO","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6VL_arA54uEZ","colab_type":"code","colab":{}},"source":["\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qtJTOTZ644l-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dz1EqZwB459t","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nAfeeKml5AVL","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q9bY61jYJNZN","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}