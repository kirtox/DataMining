{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"Copy of Final Spark Milb Tutorial 2019: PM25 Prediction.ipynb","provenance":[{"file_id":"1Cze6yWhLwD68bpqs7QDYDVEjdXyxJtvz","timestamp":1577949557668},{"file_id":"1lSX0LKOj4BNBYOA7cEK_XNcmUpheoqhD","timestamp":1577694369919},{"file_id":"1A1B42qm4_YpnUEGThH_0X__sUYMSvgcr","timestamp":1577344553323},{"file_id":"1nJQKp1ZGv1yJF6_Ov4QNTA76a_sfQNQa","timestamp":1576650253656}],"collapsed_sections":[]},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"smAgUJKT6lvw","colab_type":"text"},"source":["# 環境設定"]},{"cell_type":"code","metadata":{"id":"5HSCKUurUcLK","colab_type":"code","outputId":"75e7fc4f-2a3e-415e-f01a-fc2ece34a3ab","executionInfo":{"status":"ok","timestamp":1578064271569,"user_tz":-480,"elapsed":48112,"user":{"displayName":"吳韋成","photoUrl":"","userId":"04906925710104432369"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AOobZdBlRusw","colab_type":"code","outputId":"a94e8f06-ca88-404d-93b5-7d618326598b","executionInfo":{"status":"ok","timestamp":1578064288518,"user_tz":-480,"elapsed":65036,"user":{"displayName":"吳韋成","photoUrl":"","userId":"04906925710104432369"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# 環境初始化 (大約三至五分鐘)\n","! wget -O init_env.sh https://www.dropbox.com/s/6bnwn8u2hz19s59/init_env.sh && \\\n","bash init_env.sh"],"execution_count":2,"outputs":[{"output_type":"stream","text":["--2020-01-03 15:11:12--  https://www.dropbox.com/s/6bnwn8u2hz19s59/init_env.sh\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.8.1, 2620:100:601b:1::a27d:801\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.8.1|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/6bnwn8u2hz19s59/init_env.sh [following]\n","--2020-01-03 15:11:12--  https://www.dropbox.com/s/raw/6bnwn8u2hz19s59/init_env.sh\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc8fd6f85e2833a9e75753a76756.dl.dropboxusercontent.com/cd/0/inline/AvdLJPdfeskGderUIS0kb493Cw4XjcpImXjrIW4fuA8qCPfRtPcY5iVLNnEd7-vfO4Gt2naToYuTSIG8Tg3Yia3SJRatL-zxylpf4_sMqvDL9Q/file# [following]\n","--2020-01-03 15:11:12--  https://uc8fd6f85e2833a9e75753a76756.dl.dropboxusercontent.com/cd/0/inline/AvdLJPdfeskGderUIS0kb493Cw4XjcpImXjrIW4fuA8qCPfRtPcY5iVLNnEd7-vfO4Gt2naToYuTSIG8Tg3Yia3SJRatL-zxylpf4_sMqvDL9Q/file\n","Resolving uc8fd6f85e2833a9e75753a76756.dl.dropboxusercontent.com (uc8fd6f85e2833a9e75753a76756.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:601b:6::a27d:806\n","Connecting to uc8fd6f85e2833a9e75753a76756.dl.dropboxusercontent.com (uc8fd6f85e2833a9e75753a76756.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 336 [text/plain]\n","Saving to: ‘init_env.sh’\n","\n","\rinit_env.sh           0%[                    ]       0  --.-KB/s               \rinit_env.sh         100%[===================>]     336  --.-KB/s    in 0s      \n","\n","2020-01-03 15:11:13 (50.9 MB/s) - ‘init_env.sh’ saved [336/336]\n","\n","--2020-01-03 15:11:13--  https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0-bin-hadoop2.7.tgz\n","Resolving d3kbcqa49mib13.cloudfront.net (d3kbcqa49mib13.cloudfront.net)... 13.249.141.40, 13.249.141.171, 13.249.141.116, ...\n","Connecting to d3kbcqa49mib13.cloudfront.net (d3kbcqa49mib13.cloudfront.net)|13.249.141.40|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 203728858 (194M) [application/x-tar]\n","Saving to: ‘spark-2.2.0-bin-hadoop2.7.tgz’\n","\n","spark-2.2.0-bin-had 100%[===================>] 194.29M  20.1MB/s    in 10s     \n","\n","2020-01-03 15:11:23 (19.3 MB/s) - ‘spark-2.2.0-bin-hadoop2.7.tgz’ saved [203728858/203728858]\n","\n","spark-2.2.0-bin-hadoop2.7/\n","spark-2.2.0-bin-hadoop2.7/NOTICE\n","spark-2.2.0-bin-hadoop2.7/jars/\n","spark-2.2.0-bin-hadoop2.7/jars/parquet-common-1.8.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/bonecp-0.8.0.RELEASE.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-net-2.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/javax.servlet-api-3.1.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-annotations-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-hdfs-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/oro-2.0.8.jar\n","spark-2.2.0-bin-hadoop2.7/jars/xercesImpl-2.9.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/antlr-runtime-3.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/machinist_2.11-0.6.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spire_2.11-0.13.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/parquet-hadoop-1.8.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-sketch_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/stream-2.7.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/kryo-shaded-3.0.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/metrics-jvm-3.1.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-common-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/RoaringBitmap-0.5.11.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-auth-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/activation-1.1.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jta-1.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/datanucleus-core-3.2.10.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jersey-container-servlet-2.22.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jersey-guava-2.22.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jets3t-0.9.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jetty-util-6.1.26.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-compress-1.4.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jersey-container-servlet-core-2.22.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-beanutils-1.7.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hk2-utils-2.4.0-b34.jar\n","spark-2.2.0-bin-hadoop2.7/jars/parquet-format-2.3.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/avro-1.7.7.jar\n","spark-2.2.0-bin-hadoop2.7/jars/datanucleus-api-jdo-3.2.6.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jline-2.12.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/metrics-core-3.1.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/java-xmlbuilder-1.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/stax-api-1.0-2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hk2-locator-2.4.0-b34.jar\n","spark-2.2.0-bin-hadoop2.7/jars/parquet-hadoop-bundle-1.6.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jsp-api-2.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/xmlenc-0.52.jar\n","spark-2.2.0-bin-hadoop2.7/jars/xbean-asm5-shaded-4.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jackson-core-asl-1.9.13.jar\n","spark-2.2.0-bin-hadoop2.7/jars/shapeless_2.11-2.3.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-collections-3.2.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/javax.inject-1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-sql_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/json4s-jackson_2.11-3.2.11.jar\n","spark-2.2.0-bin-hadoop2.7/jars/json4s-ast_2.11-3.2.11.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-codec-1.10.jar\n","spark-2.2.0-bin-hadoop2.7/jars/leveldbjni-all-1.8.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-httpclient-3.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/aopalliance-repackaged-2.4.0-b34.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-server-common-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/minlog-1.3.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/javolution-5.5.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/datanucleus-rdbms-3.2.9.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jersey-common-2.22.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-graphx_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/api-asn1-api-1.0.0-M20.jar\n","spark-2.2.0-bin-hadoop2.7/jars/apacheds-i18n-2.0.0-M15.jar\n","spark-2.2.0-bin-hadoop2.7/jars/validation-api-1.1.0.Final.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-dbcp-1.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-pool-1.5.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-network-common_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/pmml-schema-1.2.15.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-shuffle-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/joda-time-2.9.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-hive-thriftserver_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spire-macros_2.11-0.13.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/curator-recipes-2.6.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jersey-server-2.22.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/htrace-core-3.1.0-incubating.jar\n","spark-2.2.0-bin-hadoop2.7/jars/httpclient-4.5.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-mllib-local_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/snappy-0.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/breeze-macros_2.11-0.13.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jackson-jaxrs-1.9.13.jar\n","spark-2.2.0-bin-hadoop2.7/jars/eigenbase-properties-1.1.5.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-tags_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jackson-databind-2.6.5.jar\n","spark-2.2.0-bin-hadoop2.7/jars/curator-client-2.6.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-catalyst_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/paranamer-2.6.jar\n","spark-2.2.0-bin-hadoop2.7/jars/opencsv-2.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/json4s-core_2.11-3.2.11.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hive-metastore-1.2.1.spark2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-digester-1.8.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jsr305-1.3.9.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-repl_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jetty-6.1.26.jar\n","spark-2.2.0-bin-hadoop2.7/jars/pyrolite-4.13.jar\n","spark-2.2.0-bin-hadoop2.7/jars/log4j-1.2.17.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-server-web-proxy-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/calcite-avatica-1.2.0-incubating.jar\n","spark-2.2.0-bin-hadoop2.7/jars/scala-xml_2.11-1.0.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-network-shuffle_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/calcite-linq4j-1.2.0-incubating.jar\n","spark-2.2.0-bin-hadoop2.7/jars/compress-lzf-1.0.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jtransforms-2.4.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/apache-log4j-extras-1.2.17.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jcl-over-slf4j-1.7.16.jar\n","spark-2.2.0-bin-hadoop2.7/jars/guice-3.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/gson-2.2.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/httpcore-4.4.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/protobuf-java-2.5.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-core-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-yarn_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-hive_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-lang-2.6.jar\n","spark-2.2.0-bin-hadoop2.7/jars/stax-api-1.0.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/javax.annotation-api-1.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/netty-all-4.0.43.Final.jar\n","spark-2.2.0-bin-hadoop2.7/jars/curator-framework-2.6.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-io-2.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/avro-ipc-1.7.7.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hive-beeline-1.2.1.spark2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/mesos-1.0.0-shaded-protobuf.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jackson-annotations-2.6.5.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jersey-media-jaxb-2.22.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-common-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jackson-mapper-asl-1.9.13.jar\n","spark-2.2.0-bin-hadoop2.7/jars/parquet-jackson-1.8.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-client-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jackson-module-paranamer-2.6.5.jar\n","spark-2.2.0-bin-hadoop2.7/jars/aopalliance-1.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/super-csv-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/janino-3.0.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/antlr4-runtime-4.5.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jpam-1.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-lang3-3.5.jar\n","spark-2.2.0-bin-hadoop2.7/jars/javassist-3.18.1-GA.jar\n","spark-2.2.0-bin-hadoop2.7/jars/bcprov-jdk15on-1.51.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-launcher_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/javax.ws.rs-api-2.0.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-jobclient-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/metrics-graphite-3.1.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jackson-xc-1.9.13.jar\n","spark-2.2.0-bin-hadoop2.7/jars/lz4-1.3.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/core-1.1.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-mesos_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/antlr-2.7.7.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-client-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/mx4j-3.0.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-logging-1.1.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-beanutils-core-1.8.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/libfb303-0.9.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/libthrift-0.9.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jaxb-api-2.2.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hive-exec-1.2.1.spark2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/calcite-core-1.2.0-incubating.jar\n","spark-2.2.0-bin-hadoop2.7/jars/parquet-encoding-1.8.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-mllib_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/api-util-1.0.0-M20.jar\n","spark-2.2.0-bin-hadoop2.7/jars/apacheds-kerberos-codec-2.0.0-M15.jar\n","spark-2.2.0-bin-hadoop2.7/jars/mail-1.4.7.jar\n","spark-2.2.0-bin-hadoop2.7/jars/stringtemplate-3.2.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/guice-servlet-3.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-crypto-1.0.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/JavaEWAH-0.3.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/metrics-json-3.1.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jdo-api-3.0.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/scalap-2.11.8.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hive-cli-1.2.1.spark2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/zookeeper-3.4.6.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jackson-module-scala_2.11-2.6.5.jar\n","spark-2.2.0-bin-hadoop2.7/jars/ivy-2.4.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/py4j-0.10.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/arpack_combined_all-0.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-unsafe_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/macro-compat_2.11-1.1.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jul-to-slf4j-1.7.16.jar\n","spark-2.2.0-bin-hadoop2.7/jars/pmml-model-1.2.15.jar\n","spark-2.2.0-bin-hadoop2.7/jars/parquet-column-1.8.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/javax.inject-2.4.0-b34.jar\n","spark-2.2.0-bin-hadoop2.7/jars/breeze_2.11-0.13.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-cli-1.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/chill-java-0.8.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/avro-mapred-1.7.7-hadoop2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/snappy-java-1.1.2.6.jar\n","spark-2.2.0-bin-hadoop2.7/jars/base64-2.3.8.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-compiler-3.0.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/slf4j-api-1.7.16.jar\n","spark-2.2.0-bin-hadoop2.7/jars/derby-10.12.1.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-app-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/objenesis-2.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jodd-core-3.5.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jersey-client-2.22.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/ST4-4.0.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/univocity-parsers-2.2.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/scala-parser-combinators_2.11-1.0.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jackson-core-2.6.5.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-math3-3.4.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/xz-1.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/scala-reflect-2.11.8.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-configuration-1.6.jar\n","spark-2.2.0-bin-hadoop2.7/jars/scala-compiler-2.11.8.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-api-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hk2-api-2.4.0-b34.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-core_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/guava-14.0.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/netty-3.9.9.Final.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-streaming_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/chill_2.11-0.8.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/scala-library-2.11.8.jar\n","spark-2.2.0-bin-hadoop2.7/jars/osgi-resource-locator-1.0.1.jar\n","spark-2.2.0-bin-hadoop2.7/python/\n","spark-2.2.0-bin-hadoop2.7/python/run-tests.py\n","spark-2.2.0-bin-hadoop2.7/python/test_support/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/hello/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/hello/sub_hello/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/hello/sub_hello/sub_hello.txt\n","spark-2.2.0-bin-hadoop2.7/python/test_support/hello/hello.txt\n","spark-2.2.0-bin-hadoop2.7/python/test_support/userlibrary.py\n","spark-2.2.0-bin-hadoop2.7/python/test_support/userlib-0.1.zip\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/people.json\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/people1.json\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/_SUCCESS\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/streaming/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/streaming/text-test.txt\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/text-test.txt\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/people_array.json\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/.part-r-00008.gz.parquet.crc\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/part-r-00008.gz.parquet\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_SUCCESS\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/part-r-00005.gz.parquet\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/.part-r-00005.gz.parquet.crc\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00002.gz.parquet\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00002.gz.parquet.crc\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00004.gz.parquet\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00004.gz.parquet.crc\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/.part-r-00007.gz.parquet.crc\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/part-r-00007.gz.parquet\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_metadata\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_common_metadata\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/ages_newlines.csv\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/ages.csv\n","spark-2.2.0-bin-hadoop2.7/python/test_support/SimpleHTTPServer.py\n","spark-2.2.0-bin-hadoop2.7/python/pylintrc\n","spark-2.2.0-bin-hadoop2.7/python/docs/\n","spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.ml.rst\n","spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.streaming.rst\n","spark-2.2.0-bin-hadoop2.7/python/docs/conf.py\n","spark-2.2.0-bin-hadoop2.7/python/docs/_templates/\n","spark-2.2.0-bin-hadoop2.7/python/docs/_templates/layout.html\n","spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.rst\n","spark-2.2.0-bin-hadoop2.7/python/docs/make.bat\n","spark-2.2.0-bin-hadoop2.7/python/docs/epytext.py\n","spark-2.2.0-bin-hadoop2.7/python/docs/make2.bat\n","spark-2.2.0-bin-hadoop2.7/python/docs/index.rst\n","spark-2.2.0-bin-hadoop2.7/python/docs/_static/\n","spark-2.2.0-bin-hadoop2.7/python/docs/_static/pyspark.js\n","spark-2.2.0-bin-hadoop2.7/python/docs/_static/pyspark.css\n","spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.sql.rst\n","spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.mllib.rst\n","spark-2.2.0-bin-hadoop2.7/python/docs/Makefile\n","spark-2.2.0-bin-hadoop2.7/python/.gitignore\n","spark-2.2.0-bin-hadoop2.7/python/MANIFEST.in\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/status.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/version.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/conf.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/base.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/evaluation.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/util.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/classification.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/regression.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/tests.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/tuning.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/common.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/stat.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/linalg/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/linalg/__init__.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/pipeline.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/feature.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/clustering.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/recommendation.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/__init__.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/param/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/param/_shared_params_code_gen.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/param/shared.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/param/__init__.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/fpm.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/statcounter.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/profiler.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/serializers.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/traceback_utils.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/shell.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/conf.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/session.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/window.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/tests.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/utils.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/group.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/types.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/catalog.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/context.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/dataframe.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/column.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/streaming.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/__init__.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/readwriter.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/functions.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/taskcontext.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/util.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/python/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/python/pyspark/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/python/pyspark/shell.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/daemon.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/tests.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/resultiterable.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/heapq3.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/broadcast.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/shuffle.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/cloudpickle.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/accumulators.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/java_gateway.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/util.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/listener.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/tests.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/flume.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/kafka.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/kinesis.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/dstream.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/context.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/__init__.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/context.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/storagelevel.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/__init__.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/join.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/tree.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/evaluation.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/util.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/classification.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/regression.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/tests.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/common.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/linalg/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/linalg/__init__.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/linalg/distributed.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/feature.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/clustering.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/recommendation.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/__init__.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/_statistics.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/KernelDensity.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/test.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/distribution.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/random.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/__init__.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/fpm.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/rdd.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/rddsampler.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/worker.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/files.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/find_spark_home.py\n","spark-2.2.0-bin-hadoop2.7/python/setup.cfg\n","spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/SOURCES.txt\n","spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/requires.txt\n","spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/PKG-INFO\n","spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/dependency_links.txt\n","spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/top_level.txt\n","spark-2.2.0-bin-hadoop2.7/python/run-tests\n","spark-2.2.0-bin-hadoop2.7/python/dist/\n","spark-2.2.0-bin-hadoop2.7/python/setup.py\n","spark-2.2.0-bin-hadoop2.7/python/lib/\n","spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip\n","spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip\n","spark-2.2.0-bin-hadoop2.7/python/lib/PY4J_LICENSE.txt\n","spark-2.2.0-bin-hadoop2.7/python/README.md\n","spark-2.2.0-bin-hadoop2.7/RELEASE\n","spark-2.2.0-bin-hadoop2.7/sbin/\n","spark-2.2.0-bin-hadoop2.7/sbin/start-mesos-shuffle-service.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/start-mesos-dispatcher.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/spark-daemon.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/stop-slaves.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/stop-thriftserver.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/stop-shuffle-service.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/stop-history-server.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/spark-config.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/start-history-server.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/start-thriftserver.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/start-shuffle-service.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/spark-daemons.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/start-all.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/stop-master.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/stop-mesos-dispatcher.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/stop-slave.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/start-slave.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/stop-mesos-shuffle-service.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/start-slaves.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/stop-all.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/slaves.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/start-master.sh\n","spark-2.2.0-bin-hadoop2.7/examples/\n","spark-2.2.0-bin-hadoop2.7/examples/src/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/RSparkSQLExample.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/fpm.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/gbt.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/isoreg.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/randomForest.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/als.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/kstest.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/gaussianMixture.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/ml.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/survreg.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/glm.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/kmeans.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/naiveBayes.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/svmLinear.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/lda.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/bisectingKmeans.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/mlp.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/logit.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/data-manipulation.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/dataframe.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/streaming/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/streaming/structured_network_wordcount.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/status_api_demo.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/linearsvc.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/random_forest_regressor_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/stopwords_remover_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/min_hash_lsh_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/standard_scaler_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/fpgrowth_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/vector_assembler_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/max_abs_scaler_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/binarizer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/imputer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/bisecting_k_means_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/cross_validator.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_summary_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/dataframe_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/naive_bayes_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/generalized_linear_regression_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_regression_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/count_vectorizer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/one_vs_rest_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/linear_regression_with_elastic_net.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/index_to_string_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/sql_transformer.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/correlation_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/dct_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/isotonic_regression_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/vector_slicer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/multilayer_perceptron_classification.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/kmeans_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/pca_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/lda_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/polynomial_expansion_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/onehot_encoder_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/aft_survival_regression.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/rformula_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_classification_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/estimator_transformer_param_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/tokenizer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/min_max_scaler_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/pipeline_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/random_forest_classifier_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/string_indexer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/als_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/n_gram_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/vector_indexer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/normalizer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/quantile_discretizer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/train_validation_split.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/chisq_selector_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/gaussian_mixture_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/bucketizer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/elementwise_product_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/tf_idf_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/word2vec_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/chi_square_test_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/pagerank.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/hive.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/basic.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/datasource.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/pi.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/logistic_regression.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/network_wordjoinsentiments.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/sql_network_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/queue_stream.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/network_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/kafka_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/stateful_network_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/direct_kafka_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/hdfs_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/recoverable_network_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/flume_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/transitive_closure.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/kmeans.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/avro_inputformat.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/linear_regression_with_sgd_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/svd_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/recommendation_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/regression_metrics_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/standard_scaler_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/kernel_density_estimation_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_regression_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/sampled_rdds.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/fpgrowth_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/streaming_k_means_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/bisecting_k_means_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_classification_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/multi_class_metrics_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/correlations_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/pca_rowmatrix_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/naive_bayes_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_regression_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/ranking_metrics_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_model.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_regression_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/power_iteration_clustering_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/binary_classification_metrics_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/isotonic_regression_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/word2vec.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/summary_statistics_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/multi_label_metrics_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/kmeans.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_classification_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/random_rdd_generation.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/svm_with_sgd_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/streaming_linear_regression_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/stratified_sampling_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/normalizer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_classification_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/elementwise_product_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/tf_idf_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/word2vec_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/correlations.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/k_means_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/parquet_inputformat.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/als.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sort.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaTC.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaFlumeEventCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaKafkaWordCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRegressionMetricsExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLinearRegressionWithSGDExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/FlumePollingEventCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/FlumeEventCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/KafkaWordCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RegressionMetricsExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LinearRegressionWithSGDExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LinearRegression.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/people.json\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/employees.json\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/people.txt\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/full_user.avsc\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/kv1.txt\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/users.parquet\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/user.avsc\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/users.avro\n","spark-2.2.0-bin-hadoop2.7/examples/jars/\n","spark-2.2.0-bin-hadoop2.7/examples/jars/scopt_2.11-3.3.0.jar\n","spark-2.2.0-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/data/\n","spark-2.2.0-bin-hadoop2.7/data/graphx/\n","spark-2.2.0-bin-hadoop2.7/data/graphx/followers.txt\n","spark-2.2.0-bin-hadoop2.7/data/graphx/users.txt\n","spark-2.2.0-bin-hadoop2.7/data/streaming/\n","spark-2.2.0-bin-hadoop2.7/data/streaming/AFINN-111.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/\n","spark-2.2.0-bin-hadoop2.7/data/mllib/pagerank_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/kmeans_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/streaming_kmeans_data_test.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_lda_libsvm_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_kmeans_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/pic_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_isotonic_regression_libsvm_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/als/\n","spark-2.2.0-bin-hadoop2.7/data/mllib/als/test.data\n","spark-2.2.0-bin-hadoop2.7/data/mllib/als/sample_movielens_ratings.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_fpgrowth.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/ridge-data/\n","spark-2.2.0-bin-hadoop2.7/data/mllib/ridge-data/lpsa.data\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_multiclass_classification_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_linear_regression_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_binary_classification_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_lda_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_movielens_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_svm_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/gmm_data.txt\n","spark-2.2.0-bin-hadoop2.7/R/\n","spark-2.2.0-bin-hadoop2.7/R/lib/\n","spark-2.2.0-bin-hadoop2.7/R/lib/sparkr.zip\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/groupBy.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/covar_pop.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sampleBy.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sql.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/year.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tan.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GBTRegressionModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/last_day.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sign.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hint.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/randn.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/orderBy.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/otherwise.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/AFTSurvivalRegressionModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hashCode.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/bin.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dim.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.svmLinear.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/minute.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/createExternalTable-deprecated.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/distinct.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.conf.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/print.jobj.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/md5.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cbrt.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.ml.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dapplyCollect.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/acos.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tables.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/NaiveBayesModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sum.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/structType.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hex.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/isLocal.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.jdbc.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/from_utc_timestamp.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tableNames.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/createDataFrame.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/isStreaming.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/toJSON.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.getSparkFiles.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/except.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/LDAModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/months_between.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark_partition_id.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.parquet.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sumDistinct.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/awaitTermination.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/BisectingKMeansModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sha2.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/abs.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/date_format.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/withColumn.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dayofyear.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sort_array.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/storageLevel.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/setCurrentDatabase.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ceil.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/floor.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/stddev_pop.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sd.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/print.structType.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.survreg.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/predict.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/count.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/unhex.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/mean.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/instr.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/from_unixtime.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/saveAsTable.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ltrim.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkRHive.init-deprecated.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.parquet.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/match.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/is.nan.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.ml.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lag.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.json.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/unpersist.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/corr.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.callJStatic.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rint.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/LinearSVCModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.gbt.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/RandomForestClassificationModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/persist.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/var_pop.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/selectExpr.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/crc32.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/expr.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.callJMethod.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/with.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/generateAliasesForIntersectedCols.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GeneralizedLinearRegressionModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/IsotonicRegressionModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/setLogLevel.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/shiftRightUnsigned.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/base64.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/array_contains.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/expm1.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.orc.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.version.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/insertInto.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/SparkDataFrame.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/merge.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dayofmonth.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/listDatabases.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/summarize.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/format_number.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/from_json.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dropDuplicates.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cache.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.text.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/approxCountDistinct.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkRSQL.init-deprecated.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/LogisticRegressionModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/stddev_samp.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/pivot.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/showDF.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/between.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/struct.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/subset.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/posexplode.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lit.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/shiftLeft.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/glm.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hypot.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/recoverPartitions.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.session.stop.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/translate.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/drop.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GBTClassificationModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/shiftRight.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/regexp_replace.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/randomSplit.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/length.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rowsBetween.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.jdbc.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/schema.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/toRadians.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/filter.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/bround.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/createOrReplaceTempView.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cancelJobGroup.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/second.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/upper.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/head.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/limit.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/concat_ws.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/when.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/FPGrowthModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/install.spark.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.newJObject.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dropTempView.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/unbase64.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/soundex.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/structField.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.addFile.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.bisectingKmeans.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cacheTable.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cosh.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.mlp.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ntile.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/atan2.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.kstest.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dtypes.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/reverse.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sinh.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.lda.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/createTable.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/negate.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/asin.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hash.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rank.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/toDegrees.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/columns.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/columnfunctions.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/substring_index.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/to_timestamp.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.naiveBayes.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/atan.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.isoreg.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/factorial.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/countDistinct.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/quarter.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/setCheckpointDir.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/least.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.text.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/windowOrderBy.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dapply.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/coalesce.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/refreshByPath.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cume_dist.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dense_rank.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/freqItems.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/getNumPartitions.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/KMeansModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.session.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/arrange.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.stream.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/encode.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.glm.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/isActive.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/crossJoin.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rpad.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/uncacheTable.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/size.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/conv.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/log10.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/collect.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.stream.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/format_string.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/windowPartitionBy.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/union.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/stopQuery.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dropTempTable-deprecated.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/endsWith.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/startsWith.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/nanvl.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/mutate.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/explain.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/R.css\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cov.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/var_samp.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/log2.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/registerTempTable-deprecated.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lastProgress.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/attach.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/min.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ncol.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/month.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/window.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/partitionBy.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/percent_rank.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/listFunctions.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/to_utc_timestamp.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/crosstab.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/take.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/exp.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/to_json.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/column.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ifelse.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GaussianMixtureModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.logit.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/show.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/setJobGroup.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/KSTest-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/clearJobGroup.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/last.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/printSchema.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rename.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rbind.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/over.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/MultilayerPerceptronClassificationModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/coltypes.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/datediff.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lead.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/summary.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/WindowSpec.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/unix_timestamp.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tanh.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/listColumns.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/to_date.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/00Index.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.uiWebUrl.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/max.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/var.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/print.structField.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.als.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/alias.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sha1.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/status.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.orc.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/currentDatabase.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.df.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/round.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.init-deprecated.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/refreshTable.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/nrow.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/pmod.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/intersect.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rtrim.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/substr.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/log.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/levenshtein.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/monotonically_increasing_id.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/checkpoint.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/decode.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.lapply.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.json.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/RandomForestRegressionModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/trim.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/select.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/regexp_extract.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/as.data.frame.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rangeBetween.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/queryName.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sample.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lower.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/repartition.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/concat.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cast.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/date_add.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.fpGrowth.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hour.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/initcap.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/explode.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/add_months.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/bitwiseNOT.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/approxQuantile.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/kurtosis.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/greatest.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/first.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.df.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rand.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/next_day.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/clearCache.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/nafunctions.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/row_number.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lpad.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/skewness.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/gapplyCollect.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/locate.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/avg.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sqrt.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cos.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/log1p.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/str.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/StreamingQuery.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ALSModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ascii.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/listTables.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sin.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/histogram.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/weekofyear.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GroupedData.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/fitted.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/date_sub.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tableToDF.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.getSparkFilesRootDirectory.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/join.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/gapply.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.randomForest.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.kmeans.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.gaussianMixture.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/INDEX\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/R/\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdx\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/R/SparkR\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdb\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/aliases.rds\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/paths.rds\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdx\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/AnIndex\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdb\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/DESCRIPTION\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/hsearch.rds\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/nsInfo.rds\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/package.rds\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/links.rds\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/Rd.rds\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/profile/\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/profile/general.R\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/profile/shell.R\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/worker/\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/worker/daemon.R\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/worker/worker.R\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/NAMESPACE\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/tests/\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/tests/testthat/\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/tests/testthat/test_basic.R\n","spark-2.2.0-bin-hadoop2.7/licenses/\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-slf4j.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-scalacheck.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-py4j.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-paranamer.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-netlib.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-Mockito.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-minlog.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-graphlib-dot.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-d3.min.js.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-f2j.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-junit-interface.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-boto.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-spire.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-pyrolite.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-xmlenc.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-scala.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-SnapTree.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-AnchorJS.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-jline.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-heapq.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-cloudpickle.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-reflectasm.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-jpmml-model.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-modernizr.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-javolution.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-jbcrypt.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-sbt-launch-lib.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-postgresql.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-kryo.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-DPark.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-scopt.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-sorttable.js.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-dagre-d3.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-jquery.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-protobuf.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-antlr.txt\n","spark-2.2.0-bin-hadoop2.7/conf/\n","spark-2.2.0-bin-hadoop2.7/conf/fairscheduler.xml.template\n","spark-2.2.0-bin-hadoop2.7/conf/metrics.properties.template\n","spark-2.2.0-bin-hadoop2.7/conf/spark-env.sh.template\n","spark-2.2.0-bin-hadoop2.7/conf/log4j.properties.template\n","spark-2.2.0-bin-hadoop2.7/conf/docker.properties.template\n","spark-2.2.0-bin-hadoop2.7/conf/slaves.template\n","spark-2.2.0-bin-hadoop2.7/conf/spark-defaults.conf.template\n","spark-2.2.0-bin-hadoop2.7/LICENSE\n","spark-2.2.0-bin-hadoop2.7/bin/\n","spark-2.2.0-bin-hadoop2.7/bin/spark-shell\n","spark-2.2.0-bin-hadoop2.7/bin/spark-submit.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/spark-shell2.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/pyspark\n","spark-2.2.0-bin-hadoop2.7/bin/sparkR.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/spark-class2.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/run-example.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/spark-submit2.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/spark-class\n","spark-2.2.0-bin-hadoop2.7/bin/spark-submit\n","spark-2.2.0-bin-hadoop2.7/bin/spark-sql\n","spark-2.2.0-bin-hadoop2.7/bin/find-spark-home\n","spark-2.2.0-bin-hadoop2.7/bin/run-example\n","spark-2.2.0-bin-hadoop2.7/bin/beeline\n","spark-2.2.0-bin-hadoop2.7/bin/pyspark2.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/spark-shell.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/spark-class.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/pyspark.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/sparkR\n","spark-2.2.0-bin-hadoop2.7/bin/beeline.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/sparkR2.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/load-spark-env.sh\n","spark-2.2.0-bin-hadoop2.7/bin/load-spark-env.cmd\n","spark-2.2.0-bin-hadoop2.7/yarn/\n","spark-2.2.0-bin-hadoop2.7/yarn/spark-2.2.0-yarn-shuffle.jar\n","spark-2.2.0-bin-hadoop2.7/README.md\n","環境初始化完畢\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xG7nN2zyUYYp","colab_type":"code","colab":{}},"source":["import os, sys\n","os.environ['SPARK_HOME'] = \"/usr/local/spark\"\n","os.environ['PYSPARK_PYTHON'] = \"/usr/local/bin/python\"\n","sys.path.append(\"/usr/local/spark/python/\")\n","sys.path.append(\"/usr/local/spark/python/lib/pyspark.zip\")\n","sys.path.append(\"/usr/local/spark/python/lib/py4j-0.10.4-src.zip\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NIbNGbuKVKCt","colab_type":"code","outputId":"d9614b12-3222-4e00-de0b-2d2486d7d94a","executionInfo":{"status":"ok","timestamp":1578064296464,"user_tz":-480,"elapsed":72951,"user":{"displayName":"吳韋成","photoUrl":"","userId":"04906925710104432369"}},"colab":{"base_uri":"https://localhost:8080/","height":374}},"source":["!apt-get update"],"execution_count":4,"outputs":[{"output_type":"stream","text":["\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n","\r0% [Waiting for headers] [Waiting for headers] [1 InRelease 0 B/3,626 B 0%] [Wa\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Waiting f\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rGet:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","\r                                                                               \rHit:5 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [4 InRelease 14.2 kB/88.7 k\r                                                                               \rHit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n","\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [4 InRelease 14.2 kB/88.7 k\r                                                                               \rHit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [4 InRelease 14.2 kB/88.7 k\r                                                                               \rGet:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]\n","\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [4 InRelease 14.2 kB/88.7 k\r                                                                               \rGet:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]\n","\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [4 InRelease 14.2 kB/88.7 k\r                                                                               \rGet:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","\r0% [1 InRelease gpgv 3,626 B] [10 InRelease 14.2 kB/88.7 kB 16%] [4 InRelease 2\r0% [10 InRelease 15.6 kB/88.7 kB 18%] [4 InRelease 43.1 kB/88.7 kB 49%] [Waitin\r0% [5 InRelease gpgv 21.3 kB] [10 InRelease 15.6 kB/88.7 kB 18%] [4 InRelease 4\r                                                                               \rGet:11 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n","\r0% [5 InRelease gpgv 21.3 kB] [10 InRelease 15.6 kB/88.7 kB 18%] [4 InRelease 4\r                                                                               \rGet:12 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [81.6 kB]\n","\r0% [5 InRelease gpgv 21.3 kB] [10 InRelease 15.6 kB/88.7 kB 18%] [4 InRelease 4\r0% [5 InRelease gpgv 21.3 kB] [10 InRelease 15.6 kB/88.7 kB 18%] [4 InRelease 5\r0% [5 InRelease gpgv 21.3 kB] [10 InRelease 47.5 kB/88.7 kB 54%] [4 InRelease 6\r0% [12 Packages store 0 B] [5 InRelease gpgv 21.3 kB] [10 InRelease 47.5 kB/88.\r0% [5 InRelease gpgv 21.3 kB] [10 InRelease 47.5 kB/88.7 kB 54%] [4 InRelease 6\r                                                                               \r0% [5 InRelease gpgv 21.3 kB] [10 InRelease 47.5 kB/88.7 kB 54%]\r                                                                \r0% [5 InRelease gpgv 21.3 kB] [Waiting for headers]\r                                                   \r0% [Waiting for headers]\r0% [6 InRelease gpgv 242 kB] [Waiting for headers]\r                                                  \rGet:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Get:15 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [30.4 kB]\n","Get:16 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,749 kB]\n","Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [761 kB]\n","Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,057 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,322 kB]\n","Get:20 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [844 kB]\n","Fetched 6,118 kB in 3s (1,988 kB/s)\n","Reading package lists... Done\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KUfG3ds3UacR","colab_type":"code","colab":{}},"source":["from pyspark import SparkContext\n","from pyspark import SparkConf\n","\n","sc = SparkContext()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EIye8lF5Gp5x","colab_type":"text"},"source":["# --------預處理部分--------\n"]},{"cell_type":"markdown","metadata":{"id":"TftpsPXb6qlh","colab_type":"text"},"source":["# 讀取資料"]},{"cell_type":"code","metadata":{"id":"7b29Tbg-Ueh3","colab_type":"code","outputId":"4d3b571f-8f37-47b6-d50d-131f2f1d17ac","executionInfo":{"status":"ok","timestamp":1578064381083,"user_tz":-480,"elapsed":6236,"user":{"displayName":"吳韋成","photoUrl":"","userId":"04906925710104432369"}},"colab":{"base_uri":"https://localhost:8080/","height":357}},"source":["!wget -O pm25.csv \"https://www.dropbox.com/s/zkn3ba7pitv83el/pm2.5Taiwan.csv?dl=0\""],"execution_count":6,"outputs":[{"output_type":"stream","text":["--2020-01-03 15:12:56--  https://www.dropbox.com/s/zkn3ba7pitv83el/pm2.5Taiwan.csv?dl=0\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.8.1, 2620:100:6018:1::a27d:301\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.8.1|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/zkn3ba7pitv83el/pm2.5Taiwan.csv [following]\n","--2020-01-03 15:12:56--  https://www.dropbox.com/s/raw/zkn3ba7pitv83el/pm2.5Taiwan.csv\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc1e8f506092f22e64da2e351892.dl.dropboxusercontent.com/cd/0/inline/AvdWcXqNuj4ys7F_EdqRxFt_TMwdDOsjf_id8xTLzD5w4LbBIaEH8nihRffeINWItmYzONFQMxC8M6DktDtR9gKWL3VrK7Vuh_KIlku6Z3yTWQ/file# [following]\n","--2020-01-03 15:12:56--  https://uc1e8f506092f22e64da2e351892.dl.dropboxusercontent.com/cd/0/inline/AvdWcXqNuj4ys7F_EdqRxFt_TMwdDOsjf_id8xTLzD5w4LbBIaEH8nihRffeINWItmYzONFQMxC8M6DktDtR9gKWL3VrK7Vuh_KIlku6Z3yTWQ/file\n","Resolving uc1e8f506092f22e64da2e351892.dl.dropboxusercontent.com (uc1e8f506092f22e64da2e351892.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:601b:6::a27d:806\n","Connecting to uc1e8f506092f22e64da2e351892.dl.dropboxusercontent.com (uc1e8f506092f22e64da2e351892.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 50453822 (48M) [text/plain]\n","Saving to: ‘pm25.csv’\n","\n","pm25.csv            100%[===================>]  48.12M  47.6MB/s    in 1.0s    \n","\n","2020-01-03 15:12:58 (47.6 MB/s) - ‘pm25.csv’ saved [50453822/50453822]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M09-xvGPRus1","colab_type":"code","colab":{}},"source":["weather_data = sc.textFile(\"pm25.csv\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1nyYKWAlRus3","colab_type":"code","colab":{}},"source":["weather_data_rdd = weather_data.map(lambda line : line.split(\",\"))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7-shIXDJRus6","colab_type":"code","outputId":"91cb4a31-e523-4c9a-9a5a-71a3e8951da4","executionInfo":{"status":"ok","timestamp":1578064381893,"user_tz":-480,"elapsed":6280,"user":{"displayName":"吳韋成","photoUrl":"","userId":"04906925710104432369"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["pm25schema = weather_data_rdd.first()\n","print (pm25schema)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["['日期', '測站', '測項', '00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ICnDa8u3S8RV","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4rt1uTvJRus8","colab_type":"code","colab":{}},"source":["import math\n","def remove_row_with_noise (x):\n","    for i in range(3, len(x)):\n","        if not x[i].isdecimal():\n","            return False\n","    return True "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fEmVln3vRus-","colab_type":"code","colab":{}},"source":["clean_weather_data = weather_data_rdd\\\n","                    .filter(lambda x: x!=pm25schema)\\\n","                    .filter(remove_row_with_noise)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YPO6sPCbRs9L","colab_type":"code","colab":{}},"source":["#print(set(clean_weather_data.map(lambda x : x[1]).collect()))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vc9koXVa3AAa","colab_type":"code","colab":{}},"source":["list1 = list(set(clean_weather_data.map(lambda x : x[1]).collect()))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lu6Ebd9O33aW","colab_type":"code","colab":{}},"source":["#str1 =''\n","#for i in range(75):\n","#  str2 = \"x[1] == '\" + list1[i] + \"' or \"\n","#  str1 += str2\n","#print(str1[:-3])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"28gvryVQ0VZ8","colab":{}},"source":["#len(set(clean_weather_data.map(lambda x : x[1]).collect()))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zopn34l3RutA","colab_type":"code","colab":{}},"source":["dalipm25 = clean_weather_data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3b1mVdPPKCtP","colab_type":"code","colab":{}},"source":["#dalipm25 = clean_weather_data.filter(lambda x :x[1] == '士林' or x[1] == '埔里' or x[1] == '小港' or x[1] == '觀音' or x[1] == '花蓮' or x[1] == '松山' or x[1] == '新竹' or x[1] == '陽明' or x[1] == '新莊' or x[1] == '安南' or x[1] == '忠明' or x[1] == '屏東' or x[1] == '平鎮' or x[1] == '頭份' or x[1] == '金門' or x[1] == '前金' or x[1] == '中山' or x[1] == '善化' or x[1] == '崙背' or x[1] == '淡水' or x[1] == '林園' or x[1] == '冬山' or x[1] == '馬公' or x[1] == '鳳山' or x[1] == '沙鹿' or x[1] == '潮州' or x[1] == '菜寮' or x[1] == '新營' or x[1] == '新店' or x[1] == '仁武' or x[1] == '臺西' or x[1] == '彰化' or x[1] == '三義' or x[1] == '汐止' or x[1] == '苗栗' or x[1] == '關山' or x[1] == '大里' or x[1] == '湖口' or x[1] == '南投' or x[1] == '馬祖' or x[1] == '宜蘭' or x[1] == '板橋' or x[1] == '美濃' or x[1] == '橋頭' or x[1] == '斗六' or x[1] == '復興' or x[1] == '大寮' or x[1] == '楠梓' or x[1] == '龍潭' or x[1] == '大同' or x[1] == '豐原' or x[1] == '朴子' or x[1] == '嘉義' or x[1] == '麥寮' or x[1] == '桃園' or x[1] == '新港' or x[1] == '左營' or x[1] == '基隆' or x[1] == '土城' or x[1] == '古亭' or x[1] == '中壢' or x[1] == '永和' or x[1] == '萬里' or x[1] == '西屯' or x[1] == '前鎮' or x[1] == '二林' or x[1] == '林口' or x[1] == '臺南' or x[1] == '線西' or x[1] == '臺東' or x[1] == '大園' or x[1] == '三重' or x[1] == '竹東' or x[1] == '恆春' or x[1] == '萬華' )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bg7Rz5vcRutC","colab_type":"code","colab":{}},"source":["from pyspark.sql import SQLContext\n","from pyspark.sql import Row\n","dalipm25row = dalipm25.map(lambda p:\n","        Row(\n","        date = p[0],\n","        location = p[1],\n","        measure = p[2],\n","        hr_01 = float(p[3]),hr_02 = float(p[4]),hr_03 = float(p[5]),hr_04 = float(p[6]),hr_05 = float(p[7]),\n","        hr_06 = float(p[8]),hr_07 = float(p[9]),hr_08 = float(p[10]),hr_09 = float(p[11]),hr_10 = float(p[12]),\n","        hr_11 = float(p[13]),hr_12 = float(p[14]),hr_13 = float(p[15]),hr_14 = float(p[16]),hr_15 = float(p[17]),\n","        hr_16 = float(p[18]),hr_17 = float(p[19]),hr_18 = float(p[20]),hr_19 = float(p[21]),hr_20 = float(p[22]),\n","        hr_21 = float(p[23]),hr_22 = float(p[24]),hr_23 = float(p[25]),hr_24 = float(p[26]),\n","    )\n",")\n","\n","\n","#dalipm25row.take(10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cLytXM3SVl0u","colab_type":"code","colab":{}},"source":["from pyspark.sql import SQLContext\n","from pyspark.sql import Row\n","from pyspark.sql import SparkSession\n","spark = SparkSession \\\n","    .builder \\\n","    .getOrCreate()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UTF_1fjPRutE","colab_type":"code","colab":{}},"source":["df = spark.createDataFrame(dalipm25row)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zYCzeJkiRutG","colab_type":"code","colab":{}},"source":["#df.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hkf4EjAFa-SO","colab_type":"code","colab":{}},"source":["schemea = [\"date\", \"location\", \"measure\", \n","           \"hr_01\", \"hr_02\", \"hr_03\", \"hr_04\",\n","           \"hr_05\", \"hr_06\", \"hr_07\", \"hr_08\",\n","           \"hr_09\", \"hr_10\", \"hr_11\", \"hr_12\",\n","           \"hr_13\", \"hr_14\", \"hr_15\", \"hr_16\",\n","           \"hr_17\", \"hr_18\", \"hr_19\", \"hr_20\",\n","           \"hr_21\", \"hr_22\", \"hr_23\", \"hr_24\",           \n","          ]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xq6noPy0aGpK","colab_type":"code","colab":{}},"source":["df = dalipm25.toDF(schemea)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oxVeXmXVcUQh","colab_type":"code","colab":{}},"source":["#df.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BoQkE1Gob204","colab_type":"code","colab":{}},"source":["for i in df.columns[3:]:\n","  df = df.withColumn(i, df[i].cast(\"float\"))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tbnppctOTlxF","colab_type":"code","colab":{}},"source":["#df.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"od3Ks9FXV7_n","colab_type":"code","colab":{}},"source":["df_pm10 = df.select(\\\n","                    df.date.alias(\"datepm10\"),\\\n","                    df.hr_09.alias(\"hr_09_pm10\"),\\\n","                    df.hr_10.alias(\"hr_10_pm10\"),\\\n","                    df.hr_11.alias(\"hr_11_pm10\"),\\\n","                    df.hr_12.alias(\"hr_12_pm10\"),\\\n","                    \"measure\")\\\n","                    .filter(df.measure==\"PM10\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KjH8z-0uWOg8","colab_type":"code","colab":{}},"source":["#df_pm10.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4nlNQir4WXWR","colab_type":"code","colab":{}},"source":["df_pm25 = df.select(\\\n","                    df.date.alias(\"datepm25\"),\\\n","                    df.hr_09.alias(\"hr_09_pm25\"),\\\n","                    df.hr_10.alias(\"hr_10_pm25\"),\\\n","                    df.hr_11.alias(\"hr_11_pm25\"),\\\n","                    df.hr_12.alias(\"hr_12_pm25\"),\\\n","                    \"measure\")\\\n","                    .filter(df.measure==\"PM2.5\") \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lZT860Q4c9Nq","colab_type":"code","colab":{}},"source":["#df_pm25.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dalCcpYSRutK","colab_type":"code","colab":{}},"source":["traing_data = df_pm25\\\n","    .join(df_pm10, df_pm25.datepm25==df_pm10.datepm10)\\\n","    .drop(\"measure\")\\\n","    .drop(\"datepm10\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Gj0FE_cRutM","colab_type":"code","colab":{}},"source":["#traing_data.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OU7EIZfYRutN","colab_type":"code","colab":{}},"source":["#traing_data.corr(\"hr_09_NO2\", \"hr_09_\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7sRvAsVARutP","colab_type":"code","colab":{}},"source":["from pyspark.sql import functions as F\n","formulated_traning_data = traing_data\\\n","            .select(\"*\", F.when(traing_data.hr_12_pm25 >= 50, 1).otherwise(0))\\\n","            .drop(\"hr_12_pm10\")\\\n","            .drop(\"hr_12_pm25\")\\\n","            .drop(\"datepm25\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G7DuF9LBe3yI","colab_type":"code","colab":{}},"source":["#formulated_traning_data.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0BFqdBaURutR","colab_type":"code","colab":{}},"source":["from pyspark.sql import functions as F\n","formulated_traning_data = traing_data\\\n","            .select(\"*\", F.when(traing_data.hr_12_pm25 >= 50, 1).otherwise(0))\\\n","            .withColumnRenamed(\"CASE WHEN (hr_12_pm25 >= 50) THEN 1 ELSE 0 END\", \"pm25_condiction\")\\\n","            .drop(\"hr_12_pm10\")\\\n","            .drop(\"hr_12_pm25\")\\\n","            .drop(\"datepm25\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"URWluTXVRutT","colab_type":"code","colab":{}},"source":["#formulated_traning_data.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"375VQe-3HHd8","colab_type":"text"},"source":["# 預處理Model"]},{"cell_type":"code","metadata":{"id":"NjmMlOpqjQ3A","colab_type":"code","colab":{}},"source":["from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n","from pyspark.mllib.tree import RandomForest, RandomForestModel\n","from pyspark.mllib.util import MLUtils\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kFxFsVG648MS","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"oWWw3SYPHSUk","colab_type":"text"},"source":["# Decision Tree Model"]},{"cell_type":"code","metadata":{"id":"c1bx_NImHG5l","colab_type":"code","colab":{}},"source":["from pyspark.mllib.regression import LabeledPoint\n","\n","LabelPoints = formulated_traning_data.rdd\\\n","            .map(lambda r: LabeledPoint(r.pm25_condiction, \\\n","                                        [r.hr_09_pm25, r.hr_10_pm25, r.hr_11_pm25, \\\n","                                         r.hr_09_pm10, r.hr_10_pm10, r.hr_11_pm10 ]))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wZMq84EsHV5N","colab_type":"code","colab":{}},"source":["(trainData,validationData,testData) = LabelPoints.randomSplit([10,0,0]) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1NBweH6XHhvz","colab_type":"code","colab":{}},"source":["DTModel10 = DecisionTree.trainClassifier(trainData,\n","        numClasses=2,\n","        categoricalFeaturesInfo={},\n","        impurity=\"entropy\",\n","        maxDepth=20,\n","        maxBins=5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bdpsZ1qEHivx","colab_type":"code","colab":{}},"source":["# Save model\n","DTModel10.save(sc, \"content/drive/My Drive/colab/model/DTModel10\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j2O2LkMVH4pS","colab_type":"text"},"source":["# Random Forest Model"]},{"cell_type":"code","metadata":{"id":"AJaxcGkcH9zK","colab_type":"code","colab":{}},"source":["RFModel10 = RandomForest.trainClassifier(trainData, numClasses=2, categoricalFeaturesInfo={},\n","                                     numTrees=4, featureSubsetStrategy=\"auto\",\n","                                     impurity='entropy', maxDepth=20, maxBins=5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Qo7x7dsIPf4","colab_type":"code","colab":{}},"source":["# Save model\n","RFModel10.save(sc, \"content/drive/My Drive/colab/model/RFModel10\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DzjaXmuBIef9","colab_type":"text"},"source":["# Regression"]},{"cell_type":"code","metadata":{"id":"b8CI2PEDIRfD","colab_type":"code","colab":{}},"source":["regression_training = \\\n","             traing_data.drop(\"hr_12_pm10\")\\\n","            .drop(\"datepm25\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9UYofeUEIi4n","colab_type":"code","colab":{}},"source":["from pyspark.mllib.regression import LabeledPoint\n","\n","LabelPoints = regression_training.rdd\\\n","            .map(lambda r: LabeledPoint(r.hr_12_pm25, \\\n","                                        [r.hr_09_pm25, r.hr_10_pm25, r.hr_11_pm25, \\\n","                                         r.hr_09_pm10, r.hr_10_pm10, r.hr_11_pm10]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4VAMSH_bIlgl","colab_type":"code","colab":{}},"source":["(trainData,validationData,testData) = LabelPoints.randomSplit([10,0,0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XlF0d_kVIvKW","colab_type":"code","colab":{}},"source":["REModel10 = DecisionTree.trainRegressor(trainData, categoricalFeaturesInfo={},\n","                                    impurity='variance', maxDepth=5, maxBins=32)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ps-SyQ4tI1FV","colab_type":"code","colab":{}},"source":["# Save model\n","REModel10.save(sc, \"content/drive/My Drive/colab/model/REModel10\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WIirPqhiUmFs","colab_type":"text"},"source":["# 測資處理"]},{"cell_type":"code","metadata":{"id":"HW_pAQ6n17QC","colab_type":"code","colab":{}},"source":["#!wget \"https://www.dropbox.com/s/3tejnx4i3v2fv0v/pm%2025%20test%20samples.zip?dl=0\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oakGAFnu2gLH","colab_type":"code","colab":{}},"source":["#!unzip 'pm 25 test samples.zip?dl=0'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"McYDiAMD3NcK","colab_type":"code","colab":{}},"source":["#!ls"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SVhmIfO6OA7u","colab_type":"code","outputId":"7b756bd8-26e9-465c-efd5-2f9b1d7cb1f6","executionInfo":{"status":"ok","timestamp":1578064843568,"user_tz":-480,"elapsed":1878,"user":{"displayName":"吳韋成","photoUrl":"","userId":"04906925710104432369"}},"colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["!ls /content/drive/My\\ Drive/colab/pm25_test/"],"execution_count":53,"outputs":[{"output_type":"stream","text":["001.csv  013.csv  025.csv  037.csv  049.csv  061.csv  073.csv  085.csv\t097.csv\n","002.csv  014.csv  026.csv  038.csv  050.csv  062.csv  074.csv  086.csv\t098.csv\n","003.csv  015.csv  027.csv  039.csv  051.csv  063.csv  075.csv  087.csv\t099.csv\n","004.csv  016.csv  028.csv  040.csv  052.csv  064.csv  076.csv  088.csv\t100.csv\n","005.csv  017.csv  029.csv  041.csv  053.csv  065.csv  077.csv  089.csv\n","006.csv  018.csv  030.csv  042.csv  054.csv  066.csv  078.csv  090.csv\n","007.csv  019.csv  031.csv  043.csv  055.csv  067.csv  079.csv  091.csv\n","008.csv  020.csv  032.csv  044.csv  056.csv  068.csv  080.csv  092.csv\n","009.csv  021.csv  033.csv  045.csv  057.csv  069.csv  081.csv  093.csv\n","010.csv  022.csv  034.csv  046.csv  058.csv  070.csv  082.csv  094.csv\n","011.csv  023.csv  035.csv  047.csv  059.csv  071.csv  083.csv  095.csv\n","012.csv  024.csv  036.csv  048.csv  060.csv  072.csv  084.csv  096.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4kVNSYRsV3j5","colab_type":"code","colab":{}},"source":["def vote(choose):\n","  if sum(choose)>=2:\n","    ans = 1\n","  else:\n","    ans = 0\n","  return ans \n","\n","def strip_symbol(x):\n","  for i in range(len(x)):\n","      if x[i]==0:\n","        x[i] = \"0\"\n","      #x[i] = x[i].strip(\"-*#x\") # remove non-digits\n","      if x[i] == \"\": x[i] = \"\"\n","      if \"#\" in x[i]:x[i] = \"\" \n","      if \"*\" in x[i]:x[i] = \"\"\n","      if \"x\" in x[i]:x[i] = \"\"\n","      if \"-\" in x[i]:x[i] = \"\"\n","  return x\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lA9hhoEUOS9r","colab_type":"code","outputId":"6c2c2bc4-5ddd-469f-b7f2-7be124136c8c","executionInfo":{"status":"error","timestamp":1578064855689,"user_tz":-480,"elapsed":10609,"user":{"displayName":"吳韋成","photoUrl":"","userId":"04906925710104432369"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from pyspark.mllib.regression import LabeledPoint\n","import time\n","tt_data=[]\n","ans = \"[\"\n","total_test = 100\n","\n","for i in range(52, total_test):\n","  num = i+1\n","  print('Round: ',num)\n","  if num < 10:\n","    file_num = '/content/drive/My Drive/colab/pm25_test/00' + str(num) + '.csv'\n","    print(1)\n","  elif num >= 10 and num < 100:\n","    file_num = '/content/drive/My Drive/colab/pm25_test/0' + str(num) + '.csv'\n","    print(2)\n","  else:\n","    file_num = '/content/drive/My Drive/colab/pm25_test/' + str(num) + '.csv'\n","    print(3)\n","  print(file_num)\n","  # get start time\n","  start = time.time()\n","\n","  # read file\n","  test_data = sc.textFile(file_num)\n","  #print(test_data.count())\n","  #test_data = test_data.map(strip_symbol)\n","  test_data_rdd = test_data.map(lambda line : line.split(\",\"))\n","  test_data_rdd = test_data_rdd.filter(lambda x: x[2] == 'PM2.5' or x[2] == 'PM10')\n","\n","\n","  # get list\n","  tt_data = test_data_rdd.collect()\n"," # print(tt_data)\n","  ttt_data = tt_data[-1]  # pm25 this day\n","  ttt_pm10_data = tt_data[-2] #pm10 this day\n","  ttt_data_yes = tt_data[-3]\n","  ttt_pm10_yes = tt_data[-4]\n","  print(ttt_data)\n","  print(ttt_pm10_data)\n","  print(ttt_data_yes)\n","  print(ttt_pm10_yes)\n","  predict = 1\n","  for i in range(-1,-25,-1):\n","    if ttt_data[i] != '':\n","      predict = (i + 26) % 24 \n","      break\n","  print(predict)\n","\n","  if predict == 1:\n","    ii = 24\n","    only1data = ttt_data[ii:]\n","    print(only1_data)\n","\n","    onlypm10data = ttt_pm10_data[ii:]\n","    # data clean\n","    print(only1data)\n","    print(onlypm10data)\n","    print('after strip\\n')\n","    strip_symbol(only1data)\n","    strip_symbol(onlypm10data)\n","    for i in only1data:\n","      if i=='':\n","        if only1data.index(i) == 0:\n","          only1data[only1data.index(i)] = str( (int(only1data[only1data.index(i)+1]) + int(only1data[only1data.index(i)+2])/2) )\n","        elif only1data.index(i) == 1:\n","          only1data[only1data.index(i)] = str( (int(only1data[only1data.index(i)+1]) + int(only1data[only1data.index(i)-1])/2) )\n","        elif only1data.index(i) == 2:\n","          only1data[only1data.index(i)] = str( (int(only1data[only1data.index(i)-1]) + int(only1data[only1data.index(i)-2])/2) )\n","        \n","    for i in onlypm10data:\n","      if i=='':\n","        if onlypm10data.index(i) == 0:\n","          onlypm10data[onlypm10data.index(i)] = str( (int(onlypm10data[onlypm10data.index(i)+1]) + int(onlypm10data[onlypm10data.index(i)+2])/2) )\n","        elif onlypm10data.index(i) == 1:\n","          onlypm10data[onlypm10data.index(i)] = str( (int(onlypm10data[onlypm10data.index(i)+1]) + int(onlypm10data[onlypm10data.index(i)-1])/2) )\n","        elif onlypm10data.index(i) == 2:\n","          onlypm10data[onlypm10data.index(i)] = str( (int(onlypm10data[onlypm10data.index(i)-1]) + int(onlypm10data[onlypm10data.index(i)-2])/2) )\n","    print(only1data)\n","    print(onlypm10data)\n","    print('after clean!!')\n","    print(only1data)\n","    print(onlypm10data)\n","    only1data.extend(onlypm10data)\n","    only1data_rdd = sc.parallelize(only1data)\n","    only1data_rdd = only1data_rdd.map(lambda x: float(x))\n","    only1data_rdd.take(6)\n","    \n","  if predict == 2:\n","    only1data = []\n","    onlypm10data = []\n","    temp = ttt_data[3]\n","    only1data.append(temp)\n","    \n","    temp10 = ttt_pm10_data[3]\n","    onlypm10data.append(temp10)\n","    \n","    data_yes = ttt_data_yes[-2:]\n","    print(\"data_yes: \",type(data_yes))\n","    print(\"only1data: \",type(only1data))\n","    #temp.extend(only1data)\n","    only1data.extend(data_yes)\n","    \n","    #only1data = temp\n","    data10_yes = ttt_pm10_yes[-2:]\n","    #temp.extend(onlypm10data)\n","    onlypm10data.extend(data10_yes)\n","\n","    #onlypm10data = temp\n","    # data clean\n","    print(only1data)\n","    print(onlypm10data)\n","    print('after clean!!')\n","    strip_symbol(only1data)\n","    strip_symbol(onlypm10data)\n","    for i in only1data:\n","      if i=='':\n","        if only1data.index(i) == 0:\n","          only1data[only1data.index(i)] = str( (int(only1data[only1data.index(i)+1]) + int(only1data[only1data.index(i)+2])/2) )\n","        elif only1data.index(i) == 1:\n","          only1data[only1data.index(i)] = str( (int(only1data[only1data.index(i)+1]) + int(only1data[only1data.index(i)-1])/2) )\n","        elif only1data.index(i) == 2:\n","          only1data[only1data.index(i)] = str( (int(only1data[only1data.index(i)-1]) + int(only1data[only1data.index(i)-2])/2) )\n","        \n","    for i in onlypm10data:\n","      if i=='':\n","        if onlypm10data.index(i) == 0:\n","          onlypm10data[onlypm10data.index(i)] = str( (int(onlypm10data[onlypm10data.index(i)+1]) + int(onlypm10data[onlypm10data.index(i)+2])/2) )\n","        elif onlypm10data.index(i) == 1:\n","          onlypm10data[onlypm10data.index(i)] = str( (int(onlypm10data[onlypm10data.index(i)+1]) + int(onlypm10data[onlypm10data.index(i)-1])/2) )\n","        elif onlypm10data.index(i) == 2:\n","          onlypm10data[onlypm10data.index(i)] = str( (int(onlypm10data[onlypm10data.index(i)-1]) + int(onlypm10data[onlypm10data.index(i)-2])/2) ) \n","    print(only1data)\n","    print(onlypm10data)\n","    only1data.extend(onlypm10data)\n","    only1data_rdd = sc.parallelize(only1data)\n","    only1data_rdd = only1data_rdd.map(lambda x: float(x))\n","    only1data_rdd.take(6) \n","\n","  if predict == 3:\n","    temp = []\n","    temp10 = []\n","    only1data = ttt_data[3:5]\n","   # print('only1data' + str(only1data))\n","\n","    onlypm10data = ttt_pm10_data[3:5]\n","   # print('len', len(ttt_data_yes))\n","    last = ttt_data_yes[-1]\n","    temp.append(last)\n","    print(temp)\n","    temp.extend(only1data)\n","    only1data = temp\n","\n","    last = ttt_pm10_yes[-1]\n","    temp10.append(last)\n","    temp10.extend(onlypm10data)\n","    onlypm10data = temp\n","   # print(type(onlypm10data))\n","    # data clean\n","    print(only1data)\n","    print(onlypm10data)\n","    print('after clean!!')\n","    strip_symbol(only1data)\n","    strip_symbol(onlypm10data)\n","    for i in only1data:\n","      if i=='':\n","        if only1data.index(i) == 0:\n","          only1data[only1data.index(i)] = str( (int(only1data[only1data.index(i)+1]) + int(only1data[only1data.index(i)+2])/2) )\n","        elif only1data.index(i) == 1:\n","          only1data[only1data.index(i)] = str( (int(only1data[only1data.index(i)+1]) + int(only1data[only1data.index(i)-1])/2) )\n","        elif only1data.index(i) == 2:\n","          only1data[only1data.index(i)] = str( (int(only1data[only1data.index(i)-1]) + int(only1data[only1data.index(i)-2])/2) )\n","        \n","    for i in onlypm10data:\n","      if i=='':\n","        if onlypm10data.index(i) == 0:\n","          onlypm10data[onlypm10data.index(i)] = str( (int(onlypm10data[onlypm10data.index(i)+1]) + int(onlypm10data[onlypm10data.index(i)+2])/2) )\n","        elif onlypm10data.index(i) == 1:\n","          onlypm10data[onlypm10data.index(i)] = str( (int(onlypm10data[onlypm10data.index(i)+1]) + int(onlypm10data[onlypm10data.index(i)-1])/2) )\n","        elif onlypm10data.index(i) == 2:\n","          onlypm10data[onlypm10data.index(i)] = str( (int(onlypm10data[onlypm10data.index(i)-1]) + int(onlypm10data[onlypm10data.index(i)-2])/2) )\n","    print(only1data)\n","    print(onlypm10data)\n","    only1data.extend(onlypm10data)\n","    only1data_rdd = sc.parallelize(only1data)\n","    only1data_rdd = only1data_rdd.map(lambda x: float(x))\n","    only1data_rdd.take(6) \n","\n","    \n","  if predict > 3 :\n","    ii = predict + 2\n","    only1data = ttt_data[ii-3:ii]\n","\n","    onlypm10data = ttt_pm10_data[ii-3:ii]\n","    # data clean\n","    print(only1data)\n","    print(onlypm10data)\n","    print('after clean!!')\n","    strip_symbol(only1data)\n","    strip_symbol(onlypm10data)\n","    for i in only1data:\n","      if i=='':\n","        if only1data.index(i) == 0:\n","          only1data[only1data.index(i)] = str( (int(only1data[only1data.index(i)+1]) + int(only1data[only1data.index(i)+2])/2) )\n","        elif only1data.index(i) == 1:\n","          only1data[only1data.index(i)] = str( (int(only1data[only1data.index(i)+1]) + int(only1data[only1data.index(i)-1])/2) )\n","        elif only1data.index(i) == 2:\n","          only1data[only1data.index(i)] = str( (int(only1data[only1data.index(i)-1]) + int(only1data[only1data.index(i)-2])/2) )\n","        \n","    for i in onlypm10data:\n","      if i=='':\n","        if onlypm10data.index(i) == 0:\n","          onlypm10data[onlypm10data.index(i)] = str( (int(onlypm10data[onlypm10data.index(i)+1]) + int(onlypm10data[onlypm10data.index(i)+2])/2) )\n","        elif onlypm10data.index(i) == 1:\n","          onlypm10data[onlypm10data.index(i)] = str( (int(onlypm10data[onlypm10data.index(i)+1]) + int(onlypm10data[onlypm10data.index(i)-1])/2) )\n","        elif onlypm10data.index(i) == 2:\n","          onlypm10data[onlypm10data.index(i)] = str( (int(onlypm10data[onlypm10data.index(i)-1]) + int(onlypm10data[onlypm10data.index(i)-2])/2) )\n","    print(only1data)\n","    print(onlypm10data)\n","    only1data.extend(onlypm10data)\n","    only1data_rdd = sc.parallelize(only1data)\n","    only1data_rdd = only1data_rdd.map(lambda x: float(x))\n","    only1data_rdd.take(6)\n","\n","  \n","  only1_data = LabeledPoint(1,only1data)\n","  only1_data\n","\n","  # Decision Tree Model\n","  Load_DTModel = DecisionTreeModel.load(sc, \"content/drive/My Drive/colab/model/DTModel10\")\n","  prediction_DT = Load_DTModel.predict(only1_data.features)\n","  print('Prediction_DT: ', int(prediction_DT))\n","\n","  # Random Forest Model\n","  Load_RFModel = RandomForestModel.load(sc, \"content/drive/My Drive/colab/model/RFModel10\")\n","  prediction_RF = Load_RFModel.predict(only1_data.features)\n","  print('Prediction_RF: ', int(prediction_RF))\n","\n","  # Regression Model\n","  Load_REModel = DecisionTreeModel.load(sc, \"content/drive/My Drive/colab/model/REModel10\")\n","  prediction_data = Load_REModel.predict(only1_data.features)\n","  print('Regression Model: ', prediction_data)\n","\n","  # determine while > 50\n","  if(prediction_data>=50):\n","    prediction_RE=1\n","  else:\n","    prediction_RE=0\n","  print('Prediction_RE: ', prediction_RE)\n","\n","  choose=[]\n","  choose.append(int(prediction_DT))\n","  choose.append(int(prediction_RF))\n","  choose.append(prediction_RE)\n","  print('Choose result: ', choose)\n","  # vote result\n","  vote_result = vote(choose)\n","  print('Vote result: ', vote(choose))\n","  \n","  # update ans\n","  '''\n","  ans += str(vote_result)\n","  if num != total_test:\n","    ans += \",\"\n","  else:\n","    ans += \"]\"\n","'''\n","  # \" \"\n","  ans += \"\\\"\"\n","\n","  ans += str(vote_result)\n","  if num != total_test:\n","    ans += \"\\\",\"\n","  else:\n","    ans += \"\\\"]\"\n","\n","  # get end time\n","  end = time.time()\n","  print('Period: ' + str(round(end-start,2)) + 's')\n","  print(\"\\n\\n\")\n","  print(\"Answer: \",ans)\n","print(\"\\n##############################################################################\\n\")\n","print(\"Answer: \",ans)"],"execution_count":55,"outputs":[{"output_type":"stream","text":["Round:  53\n","2\n","/content/drive/My Drive/colab/pm25_test/053.csv\n","['2016/05/15', '大里', 'PM2.5', '17', '18', '16', '17', '12', '14', '29*', '33', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n","['2016/05/15', '大里', 'PM10', '56', '47', '38', '40', '39', '38', '33', '57', '58', '59', '43', '58', '67', '74', '69', '68', '55', '45', '51', '57', '76', '65', '61', '51']\n","['2016/05/14', '大里', 'PM2.5', '20', '24', '23', '16', '14', '17', '17', '23', '22', '27', '24', '23', '28', '28', '32', '25', '26', '25', '32', '32', '34', '28', '23', '17']\n","['2016/05/14', '大里', 'PM10', '59', '48', '41', '34', '36', '28', '26', '35', '48', '57', '56', '58', '59', '56', '45', '51', '55', '62', '60', '64', '66', '66', '61', '64']\n","9\n","['14', '29*', '33']\n","['38', '33', '57']\n","after clean!!\n","['14', '40.0', '33']\n","['38', '33', '57']\n","Prediction_DT:  0\n","Prediction_RF:  0\n","Regression Model:  32.07853462431346\n","Prediction_RE:  0\n","Choose result:  [0, 0, 0]\n","Vote result:  0\n","Period: 2.94s\n","\n","\n","\n","Answer:  [\"0\",\n","Round:  54\n","2\n","/content/drive/My Drive/colab/pm25_test/054.csv\n","['2016/05/19', '大里', 'PM2.5', '32', '24', '16', '24', '25', '25', '25', '25', '37*', '32', '30', '26', '33', '36', '34', '29', '25', '27', '28', '32', '35', '41', '', '']\n","['2016/05/19', '大里', 'PM10', '44', '45', '49', '45', '41', '48', '48', '52', '40', '52', '57', '60', '53', '51', '57', '57', '60', '59', '60', '64', '68', '77', '72', '69']\n","['2016/05/18', '大里', 'PM2.5', '10', '17', '16', '21', '22', '24', '21', '20', '21', '27', '30', '33', '35', '29', '23', '20', '25', '25', '20', '20', '25', '22', '27', '24']\n","['2016/05/18', '大里', 'PM10', '46', '42', '43', '40', '42', '43', '60', '58', '52', '45', '54', '57', '52', '43', '45', '54', '61', '65', '60', '57', '46', '36', '36', '39']\n","23\n","['32', '35', '41']\n","['64', '68', '77']\n","after clean!!\n","['32', '35', '41']\n","['64', '68', '77']\n","Prediction_DT:  0\n","Prediction_RF:  0\n","Regression Model:  41.1274872858342\n","Prediction_RE:  0\n","Choose result:  [0, 0, 0]\n","Vote result:  0\n","Period: 1.87s\n","\n","\n","\n","Answer:  [\"0\",\"0\",\n","Round:  55\n","2\n","/content/drive/My Drive/colab/pm25_test/055.csv\n","['2016/05/21', '大里', 'PM2.5', '36', '30', '35', '33', '33', '37', '40', '57', '58', '62', '60', '58', '', '', '', '', '', '', '', '', '', '', '', '']\n","['2016/05/21', '大里', 'PM10', '88', '81', '73', '73', '70', '70', '63', '80', '90', '102', '115', '127', '120', '100', '108', '126', '145', '147', '132', '129', '127', '137', '139', '105']\n","['2016/05/20', '大里', 'PM2.5', '38', '37', '31', '38', '40', '37', '27', '34', '35', '52', '46', '53', '44', '46', '', '33', '43', '36', '30', '21', '27', '37', '40', '37']\n","['2016/05/20', '大里', 'PM10', '74', '81', '89', '83', '80', '76', '66', '63', '58', '68', '81', '88', '85', '76', '', '112#', '112', '97', '89', '72', '69', '80', '87', '92']\n","13\n","['62', '60', '58']\n","['102', '115', '127']\n","after clean!!\n","['62', '60', '58']\n","['102', '115', '127']\n","Prediction_DT:  1\n","Prediction_RF:  1\n","Regression Model:  57.428741943184534\n","Prediction_RE:  1\n","Choose result:  [1, 1, 1]\n","Vote result:  1\n","Period: 1.71s\n","\n","\n","\n","Answer:  [\"0\",\"0\",\"1\",\n","Round:  56\n","2\n","/content/drive/My Drive/colab/pm25_test/056.csv\n","['2016/05/22', '大里', 'PM2.5', '49', '55', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n","['2016/05/22', '大里', 'PM10', '93', '92', '93', '81', '43', '29', '18', '15', '10', '3', '6', '12', '20', '29', '35', '33', '27', '29', '36', '38', '28', '17', '11', '8']\n","['2016/05/21', '大里', 'PM2.5', '36', '30', '35', '33', '33', '37', '40', '57', '58', '62', '60', '58', '54', '70', '81', '98', '79', '67', '63', '70', '69', '69', '48', '55']\n","['2016/05/21', '大里', 'PM10', '88', '81', '73', '73', '70', '70', '63', '80', '90', '102', '115', '127', '120', '100', '108', '126', '145', '147', '132', '129', '127', '137', '139', '105']\n","3\n","['55']\n","['55', '49', '55']\n","['55', '49', '55']\n","after clean!!\n","['55', '49', '55']\n","['55', '49', '55']\n","Prediction_DT:  0\n","Prediction_RF:  1\n","Regression Model:  53.60826319816373\n","Prediction_RE:  1\n","Choose result:  [0, 1, 1]\n","Vote result:  1\n","Period: 2.01s\n","\n","\n","\n","Answer:  [\"0\",\"0\",\"1\",\"1\",\n","Round:  57\n","2\n","/content/drive/My Drive/colab/pm25_test/057.csv\n","['2016/07/31', '大里', 'PM2.5', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n","['2016/07/31', '大里', 'PM10', '', '', '0#', '57#', '54', '42', '51', '49', '57', '51', '57', '51', '54', '53', '61', '57', '50', '48', '55', '62', '63', '57', '52', '49']\n","['2016/07/30', '大里', 'PM2.5', '16', '9', '14', '10', '13', '4', '3', '4', '15', '23', '27', '28', '22', '28', '35', '43', '41', '35', '39', '40', '', '', '', '']\n","['2016/07/30', '大里', 'PM10', '38', '27', '17', '22', '25', '28', '25', '28', '33', '33', '44', '46', '59', '56', '62', '59', '59', '56', '59', '65', '', '', '', '']\n","1\n","(1.0,[55.0,49.0,55.0,55.0,49.0,55.0])\n","['', '', '']\n","['57', '52', '49']\n","after strip\n","\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-55-a7783e171814>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0monly1data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m           \u001b[0monly1data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0monly1data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monly1data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0monly1data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monly1data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0monly1data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0monly1data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m           \u001b[0monly1data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0monly1data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monly1data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0monly1data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monly1data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0monly1data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"]}]},{"cell_type":"code","metadata":{"id":"ZWhKqJhQbIdc","colab_type":"code","colab":{}},"source":[" [\"0\",\"1\",\"0\",\"0\",\"0\",\"1\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"1\",\"0\",\"0\",\"0\",\"0\",\"1\",\"0\",\"1\",\"1\",\"1\",\"1\",\"0\",\"1\",\"0\",\"1\",\"1\",\"0\",\"1\",\"0\",\"0\",\"1\",\"1\",\"1\",\"0\",\"0\",\"1\",\"0\",\"0\",\"0\",\"0\",\"0\",\"1\",\"0\",\"0\",\"0\",\"1\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"1\",\"0\",\"0\",\"0\",\"0\",\"1\",\"0\",\"1\",\"1\",\"1\",\"1\",\"0\",\"1\",\"0\",\"1\",\"1\",\"0\",\"1\",\"0\",\"0\",\"1\",\"1\",\"1\",\"0\",\"0\",\"1\",\"0\",\"0\",\"0\"]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CZH1eBrin8Aw","colab_type":"code","colab":{}},"source":["[\"0\",\"1\",\"1\",\"0\",\"1\",\"1\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"1\",\"0\",\"1\",\"0\",\"0\",\"1\",\"0\",\"1\",\"1\",\"0\",\"0\",\"0\",\"1\",\"0\",\"1\",\"1\",\"1\",\"1\",\"0\",\"1\",\"0\",\"1\",\"1\",\"0\",\"1\",\"0\",\"0\",\"1\",\"1\",\"1\",\"0\",\"1\",\"1\",\"1\",\"1\",\"1\",\"0\",\"1\",\"0\",\"0\",\"1\",\"1\",\"0\",\"0\",\"0\",\"0\",\"1\",\"0\",\"1\",\"1\",\"0\",\"0\",\"1\",\"0\",\"1\",\"1\",\"0\",\"1\",\"0\",\"0\",\"0\",\"1\",\"0\",\"1\",\"1\",\"0\",\"0\",\"1\",\"1\",\"0\",\"1\",\"1\",\"1\",\"0\",\"0\",\"0\",\"1\",\"1\",\"0\",\"1\",\"0\",\"1\",\"1\",\"1\",\"1\",\"1\"]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ULMtPryBAmyL","colab_type":"code","outputId":"8fb0b16d-b85f-4ea5-e135-20e3791f5c05","executionInfo":{"status":"ok","timestamp":1578041876769,"user_tz":-480,"elapsed":446,"user":{"displayName":"吳韋成","photoUrl":"","userId":"04906925710104432369"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["ans = [\"0\", \"0\", \"1\", \"0\", \"0\", \"0\", \"0\", \"1\", \"0\", \"0\", \"0\", \"0\", \"1\", \"0\", \"1\", \"0\", \"0\", \"0\", \"0\", \"0\", \"1\", \"1\", \"0\", \"0\", \"1\", \"0\", \"0\", \"0\", \"0\", \"1\", \"0\", \"0\", \"0\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"0\", \"0\", \"0\", \"0\", \"1\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"1\", \"1\", \"0\", \"0\", \"1\", \"0\", \"1\", \"0\", \"1\", \"0\", \"0\", \"0\", \"1\", \"1\", \"0\", \"1\", \"1\", \"0\", \"0\", \"0\", \"0\", \"0\", \"1\", \"0\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\"]\n","#our = [\"0\",\"1\",\"0\",\"0\",\"1\",\"1\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"1\",\"1\",\"0\",\"0\",\"0\",\"1\",\"0\",\"1\",\"1\",\"1\",\"1\",\"0\",\"1\",\"0\",\"1\",\"1\",\"0\",\"1\",\"0\",\"0\",\"1\",\"1\",\"1\",\"0\",\"0\",\"1\",\"0\",\"0\",\"1\",\"0\",\"1\",\"0\",\"0\",\"1\",\"1\",\"0\",\"0\",\"0\",\"1\",\"1\",\"0\",\"1\",\"1\",\"0\",\"0\",\"1\",\"0\",\"0\",\"1\",\"0\",\"1\",\"0\",\"0\",\"0\",\"1\",\"0\",\"1\",\"0\",\"0\",\"0\",\"1\",\"1\",\"0\",\"1\",\"0\",\"0\",\"0\",\"0\",\"0\",\"1\",\"1\",\"0\",\"0\",\"0\",\"1\",\"1\",\"1\",\"0\",\"1\"]\n","our = [\"0\",\"1\",\"1\",\"0\",\"1\",\"1\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"1\",\"0\",\"1\",\"0\",\"0\",\"1\",\"0\",\"1\",\"1\",\"0\",\"0\",\"0\",\"1\",\"0\",\"1\",\"1\",\"1\",\"1\",\"0\",\"1\",\"0\",\"1\",\"1\",\"0\",\"1\",\"0\",\"0\",\"1\",\"1\",\"1\",\"0\",\"1\",\"1\",\"1\",\"1\",\"1\",\"0\",\"1\",\"0\",\"0\",\"1\",\"1\",\"0\",\"0\",\"0\",\"0\",\"1\",\"0\",\"1\",\"1\",\"0\",\"0\",\"1\",\"0\",\"1\",\"1\",\"0\",\"1\",\"0\",\"0\",\"0\",\"1\",\"0\",\"1\",\"1\",\"0\",\"0\",\"1\",\"1\",\"0\",\"1\",\"1\",\"1\",\"0\",\"0\",\"0\",\"1\",\"1\",\"0\",\"1\",\"0\",\"1\",\"1\",\"1\",\"1\",\"1\"]\n","count = 0\n","for i in range(100):\n","  if ans[i] == our[i]:\n","    count +=1\n","\n","print('Score', count)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Score 66\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c_89jtbMoB6s","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}